{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3MvGfPg9IhRT5WRXA8l+B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/MultiGPU_ExampleHorovod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo do Davide de como utilizar o HOROVOD para rodar um job de ML distribuido em multiplas GPUs "
      ],
      "metadata": {
        "id": "Eai85JO4A-eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup inicial\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os, glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "#from packaging import version\n",
        "from timeit import default_timer as timer\n",
        "import h5py\n",
        "import tensorflow.keras as keras\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from novograd import NovoGrad\n",
        "\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "\n",
        "#import nvtx.plugins.tf as nvtx_tf\n",
        "\n",
        "#import Horovod\n",
        "import horovod.tensorflow.keras as hvd"
      ],
      "metadata": {
        "id": "GlgMT1ECBTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize HOROVOD and get GPUs information"
      ],
      "metadata": {
        "id": "mTHsbw-nBlqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize Horovod\n",
        "hvd.init()\n",
        "#pin to a GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "if gpus:\n",
        "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
        "\n",
        "\n",
        "USE_XLA = True\n",
        "if USE_XLA:\n",
        "    tf.config.optimizer.set_jit(USE_XLA)\n",
        "    #reference url : https://www.tensorflow.org/xla\n",
        "\n",
        "# Profiling\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "\n",
        "if hvd.local_rank() == 0:\n",
        "   device_name = tf.test.gpu_device_name()\n",
        "   if not device_name:\n",
        "       raise SystemError('GPU device not found')\n",
        "       print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# only set `verbose` to `1` if this is the root worker. Otherwise, it should be zero.\n",
        "if hvd.rank() == 0:\n",
        "    verbose = 1\n",
        "else:\n",
        "    verbose = 0\n"
      ],
      "metadata": {
        "id": "_t1vFt6uBqcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "L-YH-4uuCQfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Arguments and Get Data"
      ],
      "metadata": {
        "id": "RyPmQ8ejC-ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Training parameters.')\n",
        "    parser.add_argument('-e', '--epochs', default=20, type=int, help='Number of training epochs.')\n",
        "    parser.add_argument('-l', '--lr_init', default=5.e-4, type=float, help='Initial learning rate.')\n",
        "    parser.add_argument('-b', '--resblocks', default=3, type=int, help='Number of residual blocks.')\n",
        "    parser.add_argument('-c', '--cuda', default=0, type=int, help='Which gpuid to use.')\n",
        "    parser.add_argument('-a', '--load_epoch', default=0, type=int, help='Which epoch to start training from')\n",
        "    parser.add_argument('-s', '--save_dir', default='MODELS', help='Directory with saved weights files')\n",
        "    parser.add_argument('-n', '--name', default='', help='Name of experiment')\n",
        "    parser.add_argument('--warmup-epochs', type=float, default=5, help='number of warmup epochs')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    lr_init = args.lr_init\n",
        "    resblocks = args.resblocks\n",
        "    epochs = args.epochs\n",
        "    expt_name = 'BoostedJets-opendata_ResNet_blocks%d_x1_epochs%d'%(resblocks, epochs)\n",
        "    expt_name = expt_name + '-' + (datetime.now().strftime(\"%Y%m%d-%H%M%S\")) + str(hvd.rank())\n",
        "    #expt_name = expt_name + '-' +  datetime.date.strftime(datetime.datetime.now(),\"%Y%m%d-%H%M%S\")\n",
        "    if len(args.name) > 0:\n",
        "        expt_name = args.name\n",
        "    if not os.path.exists('/home/u00u5ev76whwBTLvWe357/multiGPU/MODELS/' + expt_name):\n",
        "        os.mkdir('/home/u00u5ev76whwBTLvWe357/multiGPU/MODELS/' + expt_name) \n",
        "\n",
        "# Path to directory containing TFRecord files\n",
        "datafile = tf.data.Dataset.list_files('/home/u00u5ev76whwBTLvWe357/multiGPU/tfrecord_x1/*')\n",
        "#datafile = glob.glob('/home/u00u5ev76whwBTLvWe357/multiGPU/tfrecord_x1/*')\n",
        "\n"
      ],
      "metadata": {
        "id": "IPSud7MtC9PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.logs=[]\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime=time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        time_interval = time()-self.starttime\n",
        "        print(\"Time taken for epoch {} : {}\".format(epoch, time_interval))\n",
        "        self.logs.append(time_interval)    \n",
        "\n",
        "def LR_Decay(epoch):\n",
        "    drop = 0.5\n",
        "    epochs_drop = 10\n",
        "    lr = lr_init * math.pow(drop, math.floor((epoch+1)/epochs_drop))\n",
        "    return lr\n",
        "\n",
        "def restart_epoch(args):\n",
        "    epoch = 0\n",
        "    for try_epoch in range(args.epochs, 0, -1):\n",
        "        if os.path.exists(expt_name.format(epoch=try_epoch)):\n",
        "            epoch = try_epoch\n",
        "            break\n",
        "\n",
        "    return epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "UxysQU9GCSzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Dataset for HOROVOD distributed jobs ( each GPU gets a batch to process ? )"
      ],
      "metadata": {
        "id": "vePZZkqWEAzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#'''\n",
        "BATCH_SZ = 32*2 #32  #1600\n",
        "train_sz = 32*80000\n",
        "valid_sz = 32*3000  \n",
        "test_sz  = 32*20000\n",
        "#'''\n",
        "\n",
        "# Define the \n",
        "train_steps = train_sz // (BATCH_SZ*hvd.size())\n",
        "valid_steps = valid_sz // (BATCH_SZ*hvd.size())\n",
        "test_steps  = test_sz  // (BATCH_SZ*hvd.size())\n",
        "\n",
        "\n",
        "channels = [0,1,2,3,4,5,6,7]\n",
        "#channels = [0,1,2]\n",
        "granularity=1\n",
        "\n",
        "# Mapping functions used to convert tfrecords to tf dataset\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#@nvtx_tf.ops.trace(message='ExtractFromTFRecord', domain_name='DataLoading', grad_domain_name='BoostedJets')\n",
        "def extract_fn(data):\n",
        "    # extracts fields from TFRecordDataset\n",
        "    feature_description = {\n",
        "        'X_jets': tf.io.FixedLenFeature([125*granularity*125*granularity*8], tf.float32),\n",
        "        'm0': tf.io.FixedLenFeature([], tf.float32), \n",
        "        'pt': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32)\n",
        "    }\n",
        "    sample = tf.io.parse_single_example(serialized=data, features=feature_description)\n",
        "    return sample\n",
        "\n",
        "classes = 2\n",
        "def map_fn(data):\n",
        "    # reshapes X_jets, converts y to one-hot array for feeding into keras model\n",
        "    x = tf.reshape(data['X_jets'], (125*granularity,125*granularity,8))[...,0:8]\n",
        "    y = tf.one_hot(tf.cast(data['y'], tf.uint8), classes)\n",
        "    return x, y\n",
        "\n",
        "def x_fn(data):\n",
        "    return tf.reshape(data['X_jets'], (125*granularity,125*granularity,8))[...,0:8]\n",
        "\n",
        "def y_fn(data):\n",
        "    return data['y']\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "        print(\"\\n Timestamp: \"+str(tf.cast(tf.timestamp(),tf.float64)))\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "# creates training dataset containing first data_size examples in datafile\n",
        "# - datafile: name (or list of names) of TFRecord file containing training data\n",
        "#@nvtx_tf.ops.trace(message='train_dataset', domain_name='DataLoading', grad_domain_name='BoostedJets')\n",
        "def train_dataset_generator(dataset, is_training=True, batch_sz=32, columns=[0,1,2], data_size = 32*10000):\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(batch_sz * 2)\n",
        "    dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_sz, drop_remainder=True if is_training else False).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# creates training dataset containing first data_size examples in datafile\n",
        "# - datafile: name (or list of names) of TFRecord file containing training data\n",
        "#@nvtx_tf.ops.trace(message='get_dataset', domain_name='DataLoading', grad_domain_name='BoostedJets')\n",
        "def get_dataset(dataset, start, end, batch_sz=32, columns=channels):\n",
        "    dataset = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_sz, drop_remainder=False).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "#@nvtx_tf.ops.trace(message='test_dataset', domain_name='DataLoading', grad_domain_name='BoostedJets')\n",
        "def test_dataset(dataset, start, end, batch_sz=32):\n",
        "    X = dataset.map(map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_sz, drop_remainder=False)\n",
        "    Y = dataset.map(y_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(end-start)\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "#@nvtx_tf.ops.trace(message='Creat Resnet', domain_name='Resnet', grad_domain_name='BoostedJets')\n",
        "def create_resnet():\n",
        "    # Build network\n",
        "    import keras_resnet_single as networks\n",
        "    resnet = networks.ResNet.build(len(channels), resblocks, [16,32], (125*granularity,125*granularity,len(channels)), granularity)\n",
        "    # Load saved weights, if indicated\n",
        "    if args.load_epoch != 0:\n",
        "        directory = args.save_dir\n",
        "        if args.save_dir == '':\n",
        "            directory = expt_name\n",
        "        model_name = glob.glob('%s/epoch%02d-*.hdf5'%(directory, args.load_epoch))[0]\n",
        "        #assert len(model_name) == 2\n",
        "        #model_name = model_name[0].split('.hdf5')[0]+'.hdf5'\n",
        "        print('Loading weights from file:', model_name)\n",
        "        resnet.load_weights(model_name)\n",
        "    #opt = keras.optimizers.Adam(lr=lr_init, epsilon=1.e-5) # changed eps to match pytorch value\n",
        "    opt = NovoGrad(learning_rate=lr_init * hvd.size())\n",
        "    #Wrap the optimizer in a Horovod distributed optimizer -> uses hvd.DistributedOptimizer() to compute gradients.\n",
        "    opt = hvd.DistributedOptimizer(opt)\n",
        "\n",
        "    #For Horovod: We specify `experimental_run_tf_function=False` to ensure TensorFlow\n",
        "    resnet.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'], experimental_run_tf_function = False)\n",
        "    #resnet.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    if hvd.rank() == 0:\n",
        "        resnet.summary()\n",
        "    #resnet.summary()\n",
        "    return resnet\n",
        "\n",
        "def create_dataset(names):\n",
        "    return tf.data.TFRecordDataset(filenames=names, compression_type='GZIP', \n",
        "            num_parallel_reads=tf.data.experimental.AUTOTUNE).map(extract_fn, \n",
        "                    num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#def pin_memory(self):\n",
        "#    self.inp = self.inp.pin_memory()\n",
        "#    self.tgt = self.tgt.pin_memory()\n",
        "#    return self\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    decay = ''\n",
        "    #print(\">> Input file:\",datafile)\n",
        "    expt_name = '%s_%s'%(decay, expt_name)\n",
        "    for d in ['MODELS', 'METRICS']:\n",
        "        if not os.path.isdir('%s/%s'%(d, expt_name)):\n",
        "            os.makedirs('%s/%s'%(d, expt_name))\n",
        "\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_policy(policy)\n",
        "\n",
        "    #LOADING DATA\n",
        "    names = datafile\n",
        "    dataset = create_dataset(names)\n",
        "    train_data = train_dataset_generator(dataset.take(train_sz), is_training=True, batch_sz=BATCH_SZ, columns=channels, data_size=train_sz)\n",
        "    val_data = get_dataset(dataset.skip(train_sz).take(valid_sz), start=train_sz, end=train_sz+valid_sz, columns=channels)\n",
        "    test_data = test_dataset(dataset.skip(train_sz+valid_sz).take(test_sz), start=train_sz+valid_sz, end=train_sz+valid_sz+test_sz)\n"
      ],
      "metadata": {
        "id": "ttR3Nxi3Cs8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Train the Model"
      ],
      "metadata": {
        "id": "ME3RqUV4FGqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Build network\n",
        "    resnet = create_resnet()\n",
        "\n",
        "    # Model Callbacks\n",
        "    callbacks_list = []\n",
        "    callbacks_list.append(myCallback())\n",
        "    callbacks_list.append(hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=args.warmup_epochs, verbose=verbose))\n",
        "    callbacks_list.append(hvd.callbacks.LearningRateScheduleCallback(start_epoch=args.warmup_epochs, multiplier=LR_Decay))\n",
        "    callbacks_list.append(hvd.callbacks.BroadcastGlobalVariablesCallback(0))\n",
        "    callbacks_list.append(hvd.callbacks.MetricAverageCallback())\n",
        "\n",
        "    # Horovod: write logs on worker 0.\n",
        "    if hvd.rank() == 0:\n",
        "        logs = \"METRICS/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    resume_from_epoch = 0\n",
        "    #checkpointing should only be done on the root worker.\n",
        "    if hvd.rank() == 0:\n",
        "        callbacks_list.append(keras.callbacks.ModelCheckpoint('./MODELS/' + expt_name + '/epoch{epoch:02d}-{val_loss:.2f}.hdf5', verbose=verbose, save_best_only=False))#, save_weights_only=True)\n",
        "        callbacks_list.append(keras.callbacks.TensorBoard(args.save_dir))\n",
        "    resume_from_epoch = restart_epoch(args)\n",
        "    #broadcast `resume_from_epoch` from first process to all others\n",
        "    resume_from_epoch = hvd.broadcast(resume_from_epoch, 0)\n",
        "\n",
        "    history = resnet.fit(\n",
        "        train_data,\n",
        "        steps_per_epoch=train_steps,  # 80000 / hvd.size()\n",
        "        batch_size=BATCH_SZ,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=verbose if hvd.rank()==0 else 0,\n",
        "        workers=tf.data.experimental.AUTOTUNE,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=resume_from_epoch,\n",
        "        validation_data=val_data,\n",
        "        validation_steps = valid_steps)\n",
        "    \n",
        "    print('Network has finished training')\n",
        "    print(\"Running Inference\")\n",
        "    pred=resnet.predict(test_data[0],batch_size=BATCH_SZ,verbose=verbose if hvd.rank()==0 else 0, workers=tf.data.experimental.AUTOTUNE)\n",
        "    probs = pred[:,1]\n",
        "    fpr, tpr, _ = roc_curve(np.squeeze(np.array(list(test_data[1].as_numpy_iterator()))), np.squeeze(np.array(probs)))\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print('Test AUC: ' + str(roc_auc))\n",
        "    #plt.figure(1)\n",
        "    #plt.plot([0, 1], [0, 1], 'k--')\n",
        "    #plt.plot(fpr, tpr, label='Boosted Jets (area = {:.3f})'.format(roc_auc))\n",
        "    #plt.xlabel('Background rejection')\n",
        "    #plt.ylabel('Signal classification')\n",
        "    #plt.title('ROC curve')\n",
        "    #plt.legend(loc='best')\n",
        "    #plt.show()\n",
        "    "
      ],
      "metadata": {
        "id": "8pB4I0mLFK4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}