{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK0wr2onBn4d"
   },
   "source": [
    "## Deep MLP using a Chamfer Loss for Jet-Images using jet constituents in HLS data\n",
    "## Author: Andre Sznajder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3006,
     "status": "ok",
     "timestamp": 1616956528247,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "WrNOdwasBsHc",
    "outputId": "403bd6fb-ede6-43c8-9cf7-235cccf77806"
   },
   "outputs": [],
   "source": [
    "#!fusermount -u drive\n",
    "#! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1616956528520,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "MhGGIDIvBu2V",
    "outputId": "20e67e9a-104e-4ce2-ac75-540ae8e1f459"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "#data_dir = '/content/gdrive/My Drive/Colab Notebooks/Data/'\n",
    "#data_dir = '/Users/sznajder/cernbox/WorkM1/tensorflow_macos/arm64/workdir/data/hls4ml_LHCjet_30p_val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxGwUjLmBn4f"
   },
   "source": [
    "# Load train and test JetID datasets as numpy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1616956528521,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "TcXokNduBn4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_train_val ----> shape: (589600, 32, 3)\n",
      "Loaded X_test      ----> shape: (290400, 32, 3)\n",
      "Loaded Y_train_val ----> shape: (589600, 32, 3)\n",
      "Loaded Y_test      ----> shape: (290400, 32, 3)\n",
      "Flattened X_train_val ----> shape: (589600, 96)\n",
      "Flattened X_test ----> shape: (290400, 96)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "\n",
    "#Data PATH\n",
    "DATA_PATH = '/Users/sznajder/WorkM1/workdir/data/'\n",
    "\n",
    "nconstit = 32\n",
    "\n",
    "X_train_val = np.load(\"../../data/X_train_val_nconst_{}.npy\".format(nconstit))\n",
    "X_test = np.load(\"../../data/X_test_nconst_{}.npy\".format(nconstit))\n",
    "Y_train_val = np.load(\"../../data/Y_train_val_nconst_{}.npy\".format(nconstit))\n",
    "Y_test = np.load(\"../../data/Y_test_nconst_{}.npy\".format(nconstit))\n",
    "\n",
    "print(\"Loaded X_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded X_test      ----> shape:\", X_test.shape)\n",
    "print(\"Loaded Y_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded Y_test      ----> shape:\", X_test.shape)\n",
    "\n",
    "nfeat = X_train_val.shape[-1]\n",
    "\n",
    "# Flatten the 2D ( eta x phi ) jet image into 1D array to input into a MLP \n",
    "X_train_val = rearrange(  X_train_val , 'batch nconstit nfeat -> batch (nconstit nfeat)' )  \n",
    "X_test = rearrange(  X_test , 'batch nconstit nfeat -> batch (nconstit nfeat)' )  \n",
    "\n",
    "print(\"Flattened X_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Flattened X_test ----> shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ground truth is incorporated in the ['g', 'q', 'w', 'z', 't'] vector of boolean, taking the form\n",
    " \n",
    "## [1, 0, 0, 0, 0] for gluons\n",
    " \n",
    "## [0, 1, 0, 0, 0] for quarks\n",
    " \n",
    "## [0, 0, 1, 0, 0] for Ws\n",
    " \n",
    "## [0, 0, 0, 1, 0] for Zs\n",
    " \n",
    "## [0, 0, 0, 0, 1] for tops\n",
    "\n",
    "## This is what is called 'one-hot' encoding of a descrete label (typical of ground truth for classification problems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zFeYn6_Bn40"
   },
   "source": [
    "## Define our Deep MLP model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamfer Loss\n",
    "K = keras.backend\n",
    "def chamfer_loss(y_true, y_pred):           \n",
    "\n",
    "    # flatten the batch \n",
    "    y_true_f = K.batch_flatten(y_true)\n",
    "    y_pred_f = K.batch_flatten(y_pred)\n",
    "\n",
    "    y_pred_mask_f = K.sigmoid(y_pred_f - 0.5)\n",
    "\n",
    "    finalChamferDistanceSum = K.sum(y_pred_mask_f * y_true_f, axis=1, keepdims=True)  \n",
    "\n",
    "    return K.mean(finalChamferDistanceSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6350,
     "status": "ok",
     "timestamp": 1616956531619,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "aqEAX7zIBn40",
    "outputId": "f71a892c-d086-4df8-9e11-b1116e448764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type= <class 'numpy.ndarray'>\n",
      "X shape= (589600, 24)\n",
      "NINPUT =  24\n",
      "NOUTPUT =  5\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 24)]              0         \n",
      "_________________________________________________________________\n",
      "batchnorm (BatchNormalizatio (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 42)                1050      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 42)                1806      \n",
      "_________________________________________________________________\n",
      "Dense3 (Dense)               (None, 42)                1806      \n",
      "_________________________________________________________________\n",
      "DenseOut (Dense)             (None, 5)                 215       \n",
      "=================================================================\n",
      "Total params: 4,973\n",
      "Trainable params: 4,925\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# baseline keras model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import utils\n",
    "#from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(\"X type=\",type(X_train_val))\n",
    "print(\"X shape=\",X_train_val.shape)\n",
    "\n",
    "# Define MLP input layer dimension ( NINPUT = neta*nphi )\n",
    "NINPUT = len(X_train_val[1])\n",
    "NOUTPUT = len(Y_train_val[1])\n",
    "\n",
    "print(\"NINPUT = \",NINPUT)\n",
    "print(\"NOUTPUT = \",NOUTPUT)\n",
    "\n",
    "'''\n",
    "# MLP architechture for 32 contituents\n",
    "nhidden1 = int(NINPUT)\n",
    "nhidden2 = int(NINPUT)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# MLP architechture for 16 contituents\n",
    "nhidden1 = int(NINPUT)\n",
    "nhidden2 = int(NINPUT)\n",
    "'''\n",
    "\n",
    "\n",
    "# MLP architechture for 8 contituents\n",
    "#nhidden1 = int(NINPUT*2.3)\n",
    "#nhidden2 = int(NINPUT*2.3)\n",
    "\n",
    "# MLP architechture for 8 contituents\n",
    "nhidden = int(NINPUT*1.75) \n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "# Define the input tensor shape\n",
    "input  = Input(shape=(NINPUT,), name = 'input') \n",
    "hidden = BatchNormalization(name='batchnorm')(input)\n",
    "\n",
    "# Define MLP with 2 hidden layers \n",
    "hidden = Dense(nhidden,  name = 'Dense1', activation='relu')(hidden)\n",
    "hidden = Dense(nhidden,  name = 'Dense2', activation='relu')(hidden)\n",
    "hidden = Dense(nhidden,  name = 'Dense3', activation='relu')(hidden)\n",
    "\n",
    "output  = Dense(NOUTPUT,   name = 'DenseOut',  activation='softmax')(hidden)\n",
    "\n",
    "# create the model\n",
    "model = Model(inputs=input, outputs=output)\n",
    "# Define the optimizer ( minimization algorithm )\n",
    "#optim = SGD(learning_rate=0.01,decay=1e-6)\n",
    "optim = Adam(learning_rate=0.0001)\n",
    "#optim = Adam(learning_rate=0.0005)\n",
    "#optim = Adam()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optim, loss=chamfer_loss, metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "# print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlDvqPkIBn48"
   },
   "source": [
    "## Run training\n",
    "Here, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9682,
     "status": "ok",
     "timestamp": 1616956534957,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "p3lHpPv-Bn49",
    "outputId": "252bb588-0b97-49ce-c444-60926c01c2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/300\n",
      "807/807 [==============================] - 1s 961us/step - loss: 0.4120 - categorical_accuracy: 0.1229 - val_loss: 0.3944 - val_categorical_accuracy: 0.0553\n",
      "Epoch 2/300\n",
      "807/807 [==============================] - 1s 907us/step - loss: 0.3916 - categorical_accuracy: 0.0515 - val_loss: 0.3904 - val_categorical_accuracy: 0.0498\n",
      "Epoch 3/300\n",
      "807/807 [==============================] - 1s 909us/step - loss: 0.3900 - categorical_accuracy: 0.0486 - val_loss: 0.3898 - val_categorical_accuracy: 0.0485\n",
      "Epoch 4/300\n",
      "807/807 [==============================] - 1s 922us/step - loss: 0.3896 - categorical_accuracy: 0.0478 - val_loss: 0.3895 - val_categorical_accuracy: 0.0479\n",
      "Epoch 5/300\n",
      "807/807 [==============================] - 1s 973us/step - loss: 0.3894 - categorical_accuracy: 0.0472 - val_loss: 0.3894 - val_categorical_accuracy: 0.0477\n",
      "Epoch 6/300\n",
      "807/807 [==============================] - 1s 935us/step - loss: 0.3892 - categorical_accuracy: 0.0466 - val_loss: 0.3893 - val_categorical_accuracy: 0.0473\n",
      "Epoch 7/300\n",
      "807/807 [==============================] - 1s 918us/step - loss: 0.3891 - categorical_accuracy: 0.0464 - val_loss: 0.3892 - val_categorical_accuracy: 0.0471\n",
      "Epoch 8/300\n",
      "807/807 [==============================] - 1s 940us/step - loss: 0.3890 - categorical_accuracy: 0.0462 - val_loss: 0.3892 - val_categorical_accuracy: 0.0469\n",
      "Epoch 9/300\n",
      "807/807 [==============================] - 1s 966us/step - loss: 0.3890 - categorical_accuracy: 0.0461 - val_loss: 0.3891 - val_categorical_accuracy: 0.0470\n",
      "Epoch 10/300\n",
      "807/807 [==============================] - 1s 930us/step - loss: 0.3889 - categorical_accuracy: 0.0458 - val_loss: 0.3891 - val_categorical_accuracy: 0.0465\n",
      "Epoch 11/300\n",
      "807/807 [==============================] - 1s 917us/step - loss: 0.3888 - categorical_accuracy: 0.0456 - val_loss: 0.3890 - val_categorical_accuracy: 0.0466\n",
      "Saving Model :  MLP_nconst_8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# early stopping callback\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', patience=10)\n",
    "\n",
    "# Learning rate scheduler \n",
    "lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2, patience=10)\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into mlp_model.h5\n",
    "chkp = ModelCheckpoint('MLP_model.h5', monitor='val_categorical_accuracy', \n",
    "                       verbose=0, save_best_only=True, \n",
    "                       save_weights_only=False, mode='auto', \n",
    "                       period=1)\n",
    "\n",
    "\n",
    "# Train classifier\n",
    "history = model.fit(  X_train_val, Y_train_val, \n",
    "                    epochs=300, \n",
    "                    batch_size=512, \n",
    "                    verbose=1,\n",
    "                    callbacks=[lr, es, chkp], \n",
    "                    validation_split=0.3 )\n",
    "                    \n",
    "#                    callbacks=[early_stopping, model_checkpoint], \n",
    "\n",
    "# Set NN and output name\n",
    "arch = 'MLP'\n",
    "fname = arch+'_nconst_'+str(nmax)\n",
    "print('Saving Model : ',fname)\n",
    "\n",
    "\n",
    "## Save the model+ weights\n",
    "model.save(fname+'.h5')\n",
    "\n",
    "## Save the model weights in a separate file\n",
    "model.save_weights('weights'+fname+'.h5')                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np12nLtsBn5A"
   },
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "executionInfo": {
     "elapsed": 10466,
     "status": "ok",
     "timestamp": 1616956535743,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "Z_dscosMBn5B",
    "outputId": "6f9f7a67-1607-42ab-a9f8-5635b360eaec"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Plot loss vs epoch\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val loss')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# Plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "ax.plot(history.history['val_categorical_accuracy'], label='val categorical accuracy')\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "# Plot the ROC curves\n",
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "NN = {}\n",
    "NP = {}\n",
    "TP = {}\n",
    "FP = {}\n",
    "TN = {}\n",
    "FN = {}\n",
    "tresholds = {}\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "Y_predict = model.predict(X_test)\n",
    "\n",
    "# Loop over classes(labels) to get metrics per class\n",
    "for i, label in enumerate(labels):\n",
    "    fpr[label], tpr[label], tresholds[label] = roc_curve(Y_test[:,i], Y_predict[:,i])\n",
    "#    precision[label], recall[label], tresholds = precision_recall_curve(Y_test[:,i], Y_predict[:,i]) \n",
    "    print( np.unique(Y_test[:,i], return_counts=True) )\n",
    "    _ , N = np.unique(Y_test[:,i], return_counts=True) # count the NEGATIVES and POSITIVES samples in your test set\n",
    "    NN[label] = N[0]                   # number of NEGATIVES \n",
    "    NP[label] = N[1]                   # number of POSITIVES\n",
    "    TP[label] = tpr[label]*NP[label]\n",
    "    FP[label] = fpr[label]*NN[label] \n",
    "    TN[label] = NN[label] - FP[label]\n",
    "    FN[label] = NP[label] - TP[label]\n",
    "\n",
    "    auc1[label] = auc(fpr[label], tpr[label])\n",
    "    ax.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "    \n",
    "ax.semilogy()\n",
    "ax.set_xlabel(\"sig. efficiency\")\n",
    "ax.set_ylabel(\"bkg. mistag rate\")\n",
    "ax.set_ylim(0.001,1)\n",
    "#ax.set_grid(True)\n",
    "ax.legend(loc='lower right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "\n",
    "print('Tresholds=',tresholds)\n",
    "\n",
    "# Plot DNN output \n",
    "ax = plt.subplot(2, 2, 4)\n",
    "X = np.linspace(0.0, 1.0, 20)\n",
    "hist={}\n",
    "for i, name in enumerate(labels):\n",
    "    hist[name] = ax.hist(Y_predict, bins=X, label=name ,histtype='step')\n",
    "\n",
    "ax.semilogy()\n",
    "ax.set_xlabel('DNN Output')\n",
    "ax.legend(loc='lower left')\n",
    "ax.legend(prop={'size': 10})\n",
    "\n",
    "# Display plots\n",
    "fig = plt.gcf()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Save plots\n",
    "fig.savefig(fname+'.pdf')\n",
    "\n",
    "\n",
    "# Save FPR for a given TPR value ( 30% , 50% & 80%)\n",
    "with open('FPR@TPR_'+fname+'.csv', 'w') as file:\n",
    "  file.write(\"model,label,treshold,tpr,fpr\\n\")\n",
    "  for label in labels:\n",
    "    for t in [0.3, 0.5, 0.8]:\n",
    "      idx = np.argmax(tpr[label]>t)\n",
    "      file.write( arch+','+label+','+str(t)+','+str(tpr[label][idx])+','+str(fpr[label][idx])+'\\n' )\n",
    "               \n",
    "               \n",
    "# Save ROC AUC for each label\n",
    "with open('ROCAUC_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+'\\n'\n",
    "  file.write(header)\n",
    "  rocauc = str(auc1[labels[0]])+', '+str(auc1[labels[1]])+', '+str(auc1[labels[2]])+', '+str(auc1[labels[3]])+', '+str(auc1[labels[4]])\n",
    "  file.write(rocauc)\n",
    "\n",
    "\n",
    "# Save NN Accuracy for treshold of 0.5 for each label and the average over all classes\n",
    "acc_avg = float(accuracy_score (np.argmax(Y_test,axis=1), np.argmax(Y_predict,axis=1)))\n",
    "with open('ACCURACY_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+', '+'acc_avg'+'\\n'\n",
    "  file.write(header)\n",
    "  accuracy = ''\n",
    "  for label in labels:  \n",
    "    idx = np.argmax( tresholds[label] <= 0.5 )\n",
    "    accuracy += str( (TP[label][idx]+TN[label][idx])/(NP[label]+NN[label]) )+', '\n",
    "  accuracy += str(acc_avg) \n",
    "  file.write(accuracy)\n",
    "\n",
    "\n",
    "'''\n",
    "# Save confusion matrix ndarrays to .npz file\n",
    "with open('CONF_MATRIX_'+fname+'.npz', 'wb') as file:\n",
    "    vars = {}\n",
    "    vars[arch]=np.array(1) # save model name\n",
    "    for label in labels:\n",
    "        vars['tresholds_'+label+'_'+arch] = tresholds[label]\n",
    "        vars['TP_'+label+'_'+arch] = TP[label]\n",
    "        vars['FP_'+label+'_'+arch] = FP[label]\n",
    "        vars['TN_'+label+'_'+arch] = TN[label]\n",
    "        vars['FN_'+label+'_'+arch] = FN[label]\n",
    "        vars['TPR_'+arch] = tpr[label]\n",
    "        vars['FPR_'+arch] = fpr[label]\n",
    "        vars['NP_'+arch]= NP[label]\n",
    "        vars['NN_'+arch]= NN[label]\n",
    "        vars['auc_'+arch] = auc1[label] \n",
    "#        print(vars)\n",
    "    np.savez(file, **vars)\n",
    "'''\n",
    "\n",
    "\n",
    "# Save a sample of events for HLS\n",
    "njets=3000\n",
    "print(X_test.shape)\n",
    "np.save('x_test_8const.npy', X_test[0:njets,:])\n",
    "np.save('y_test_8const.npy', Y_test[0:njets,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JetTagging_MLP_TFKeras.ipynb",
   "provenance": [
    {
     "file_id": "1_LtW5Af1ruCzp6IH18NNj2K3Vbgqto-F",
     "timestamp": 1613800914899
    },
    {
     "file_id": "https://github.com/thongonary/machine_learning_vbscan/blob/master/5-conv2d.ipynb",
     "timestamp": 1551264063701
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
