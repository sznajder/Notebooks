{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgswthm9dIWWq/KeTgwLnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/LossesBasedOnDiscoverySignificance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct optimisation of the discovery significance when training neural networks to search for new physics in particle collid\n",
        "\n",
        "## https://arxiv.org/pdf/1806.00322.pdf"
      ],
      "metadata": {
        "id": "QvvugJDfH7Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "\n",
        "def findLayerSize(layer,refSize):\n",
        "\n",
        "    if isinstance(layer, float):\n",
        "        return int(layer*refSize)\n",
        "    elif isinstance(layer, int):\n",
        "        return layer\n",
        "    else:\n",
        "        print 'WARNING: layer must be int or float'\n",
        "        return None\n",
        "\n",
        "def createDenseModel(inputSize=None,outputSize=None,hiddenLayers=[1.0],dropOut=None,l2Regularization=None,activation='relu',optimizer='adam',doRegression=False,loss=None,extraMetrics=[]):\n",
        "    '''\n",
        "    Dropout: choose percentage to be dropped out on each hidden layer (not currently applied to input layer)\n",
        "    l2Regularization: choose lambda of the regularization (ie multiplier of the penalty)\n",
        "    '''\n",
        "\n",
        "    #check inputs are ok\n",
        "    assert inputSize and outputSize, 'Must provide non-zero input and output sizes'\n",
        "    assert len(hiddenLayers)>=1, 'Need at least one hidden layer'\n",
        "\n",
        "    refSize=inputSize+outputSize\n",
        "\n",
        "    #Initialise the model\n",
        "    model = Sequential()\n",
        "\n",
        "    if l2Regularization: \n",
        "        regularization=regularizers.l2(l2Regularization)\n",
        "    else:\n",
        "        regularization=None\n",
        "\n",
        "    #Add the first layer, taking the inputs\n",
        "    model.add(Dense(units=findLayerSize(hiddenLayers[0],refSize), \n",
        "        activation=activation, input_dim=inputSize,name='input',\n",
        "        kernel_regularizer=regularization))\n",
        "\n",
        "\n",
        "    if dropOut: model.add(Dropout(dropOut))\n",
        "\n",
        "    #Add the extra hidden layers\n",
        "    for layer in hiddenLayers[1:]:\n",
        "        model.add(Dense(units=findLayerSize(hiddenLayers[0],refSize), \n",
        "            activation=activation,kernel_regularizer=regularization))\n",
        "\n",
        "        if dropOut: model.add(Dropout(dropOut))\n",
        "\n",
        "    if not doRegression: # if doing a normal classification model\n",
        "\n",
        "        #Add the output layer and choose the type of loss function\n",
        "        #Choose the loss function based on whether it's binary or not\n",
        "        if outputSize==2: \n",
        "            #It's better to choose a sigmoid function and one output layer for binary\n",
        "            # This is a special case of n=2 classification\n",
        "            model.add(Dense(1, activation='sigmoid'))\n",
        "            if not loss: loss = 'binary_crossentropy'\n",
        "        else: \n",
        "            #Softmax forces the outputs to sum to 1 so the score on each node\n",
        "            # can be interpreted as the probability of getting each class\n",
        "            model.add(Dense(outputSize, activation='softmax'))\n",
        "            if not loss: loss = 'categorical_crossentropy'\n",
        "\n",
        "        #After the layers are added compile the model\n",
        "        model.compile(loss=loss,\n",
        "            optimizer=optimizer,metrics=['accuracy']+extraMetrics)\n",
        "\n",
        "    else: # if training a regression add output layer with linear activation function and mse loss\n",
        "\n",
        "        model.add(Dense(1))\n",
        "        if not loss: loss='mean_squared_error'\n",
        "        model.compile(loss=loss,\n",
        "            optimizer=optimizer,metrics=['mean_squared_error']+extraMetrics)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "def significanceLoss(expectedSignal,expectedBkgd):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def sigLoss(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "\n",
        "        return -(s*s)/(s+b+K.epsilon()) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return sigLoss\n",
        "\n",
        "def significanceLossInvert(expectedSignal,expectedBkgd):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def sigLossInvert(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "\n",
        "        return (s+b)/(s*s+K.epsilon()) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return sigLossInvert\n",
        "\n",
        "def significanceLoss2Invert(expectedSignal,expectedBkgd):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def sigLoss2Invert(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "\n",
        "        return b/(s*s+K.epsilon()) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return sigLoss2Invert\n",
        "\n",
        "def significanceLossInvertSqrt(expectedSignal,expectedBkgd):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def sigLossInvert(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "\n",
        "        return K.sqrt(s+b)/(s+K.epsilon()) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return sigLossInvert\n",
        "\n",
        "\n",
        "\n",
        "def significanceFull(expectedSignal,expectedBkgd):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def significance(y_true,y_pred):\n",
        "        #Discrete version\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(K.round(y_pred)*y_true)\n",
        "        b = bkgdWeight*K.sum(K.round(y_pred)*(1-y_true))\n",
        "\n",
        "        return s/K.sqrt(s+b+K.epsilon()) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return significance\n",
        "\n",
        "\n",
        "def asimovSignificanceLoss(expectedSignal,expectedBkgd,systematic):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def asimovSigLoss(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "        sigB=systematic*b\n",
        "\n",
        "        return -2*((s+b)*K.log((s+b)*(b+sigB*sigB)/(b*b+(s+b)*sigB*sigB+K.epsilon())+K.epsilon())-b*b*K.log(1+sigB*sigB*s/(b*(b+sigB*sigB)+K.epsilon()))/(sigB*sigB+K.epsilon())) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return asimovSigLoss\n",
        "\n",
        "def asimovSignificanceLossInvert(expectedSignal,expectedBkgd,systematic):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def asimovSigLossInvert(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(y_pred*y_true)\n",
        "        b = bkgdWeight*K.sum(y_pred*(1-y_true))\n",
        "        sigB=systematic*b\n",
        "\n",
        "        return 1./(2*((s+b)*K.log((s+b)*(b+sigB*sigB)/(b*b+(s+b)*sigB*sigB+K.epsilon())+K.epsilon())-b*b*K.log(1+sigB*sigB*s/(b*(b+sigB*sigB)+K.epsilon()))/(sigB*sigB+K.epsilon()))) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return asimovSigLossInvert\n",
        "\n",
        "def asimovSignificanceFull(expectedSignal,expectedBkgd,systematic):\n",
        "    '''Define a loss function that calculates the significance based on fixed\n",
        "    expected signal and expected background yields for a given batch size'''\n",
        "\n",
        "\n",
        "    def asimovSignificance(y_true,y_pred):\n",
        "        #Continuous version:\n",
        "\n",
        "        signalWeight=expectedSignal/K.sum(y_true)\n",
        "        bkgdWeight=expectedBkgd/K.sum(1-y_true)\n",
        "\n",
        "        s = signalWeight*K.sum(K.round(y_pred)*y_true)\n",
        "        b = bkgdWeight*K.sum(K.round(y_pred)*(1-y_true))\n",
        "        sigB=systematic*b\n",
        "\n",
        "        return K.sqrt(2*((s+b)*K.log((s+b)*(b+sigB*sigB)/(b*b+(s+b)*sigB*sigB+K.epsilon())+K.epsilon())-b*b*K.log(1+sigB*sigB*s/(b*(b+sigB*sigB)+K.epsilon()))/(sigB*sigB+K.epsilon()))) #Add the epsilon to avoid dividing by 0\n",
        "\n",
        "    return asimovSignificance\n",
        "\n",
        "\n",
        "def truePositive(y_true,y_pred):\n",
        "    return K.sum(K.round(y_pred)*y_true) / (K.sum(y_true) + K.epsilon())\n",
        "\n",
        "def falsePositive(y_true,y_pred):\n",
        "    return K.sum(K.round(y_pred)*(1-y_true)) / (K.sum(1-y_true) + K.epsilon())"
      ],
      "metadata": {
        "id": "v5P3-EIaIAER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}