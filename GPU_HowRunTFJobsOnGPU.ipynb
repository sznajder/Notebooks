{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPlfnzM8RTidIeOJr+/alL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/GPU_HowRunTFJobsOnGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo do Davide sobre como rodar um job de TF na GPU "
      ],
      "metadata": {
        "id": "N247aN464aE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment"
      ],
      "metadata": {
        "id": "t2pge3Sf5TZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import os, glob, re\n",
        "import psutil\n",
        "import GPUtil\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import scipy as sp\n",
        "import scipy.interpolate\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "matplotlib.rcParams['figure.figsize'] = [10, 10] \n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib widget\n",
        "# %matplotlib inline\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# ML border recognition\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Only needed for animations:\n",
        "import matplotlib.animation as anim\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "# Only needed to get a progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Tensorflow related packages\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "from tensorflow.keras.metrics import MeanRelativeError\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import locale\n"
      ],
      "metadata": {
        "id": "B4CtAPCf4mO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Data"
      ],
      "metadata": {
        "id": "B6tyPwlz5Yav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "folder='.'\n",
        "\n",
        "## !! only if saved!! load x_train, y_train, x_valid, y_valid from fast access\n",
        "x_train = np.load('x_train.npz')['arr_0']\n",
        "y_train = np.load('y_train.npz')['arr_0']\n",
        "x_valid = np.load('x_valid.npz')['arr_0']\n",
        "y_valid = np.load('y_valid.npz')['arr_0']\n",
        "\n",
        "print('Train shapes: x = ', x_train.shape, ' , y = ', y_train.shape)\n",
        "print('Valid shapes: x = ', x_valid.shape, ' , y = ', y_valid.shape)\n",
        "\n",
        "\n",
        "# load stats from train_stats.npz, which saved as np.savez_compressed('train_stats.npz', mean, std)\n",
        "mean = np.load('train_stats.npz')['arr_0']\n",
        "std = np.load('train_stats.npz')['arr_1']\n",
        "print(mean)\n",
        "print(std)\n",
        "\n"
      ],
      "metadata": {
        "id": "DrynQTsv5vV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get GPU information"
      ],
      "metadata": {
        "id": "_kw6nwAs5xnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set ID number of the GPU to be used\n",
        "gpu_to_use = '2'\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_to_use\n",
        "#os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/home/ddicroce/miniconda3/pkgs/cuda-nvcc-11.4.152-0\"\n",
        "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/home/ddicroce/miniconda3/envs/tf-py38\"\n",
        "\n",
        "K.clear_session()\n",
        "print(\"TF version\", tf.__version__)\n",
        "\n",
        "# Print CPU and memory information\n",
        "print(f\"CPU count: {psutil.cpu_count()}\")  # Number of CPU cores\n",
        "print(f\"Memory: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB\")  # Total system memory in GB\n",
        "\n",
        "# Print GPU information\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "gpus = GPUtil.getGPUs()\n",
        "for i, gpu in enumerate(gpus):\n",
        "    print(f\"GPU {i}: {gpu.name}, Memory: {gpu.memoryTotal / 1024:.2f} GB\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oHwRipFU5box"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Train the Model"
      ],
      "metadata": {
        "id": "SN06tw655_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout         = 1\n",
        "loss            = \"categorical_crossentropy\"\n",
        "last_activation = \"sigmoid\"\n",
        "\n",
        "def create_DFF3LC(dim):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(4096, input_dim=dim, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(2048, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(Dense(1024, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(Dense(512, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(Dense(256, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(Dense(128, activation=\"relu\"))\n",
        "  model.add(Dropout(5/100))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(32, activation=\"relu\"))\n",
        "  model.add(Dense(3, activation=last_activation))\n",
        "  return model\n",
        "\n",
        "modelname = 'DFF3LSC_4096_2048_1024_512_256_128_32_do%s_%s_%s'%(dropout, loss, last_activation)\n",
        "model = create_DFF3LC(x_train.shape[1])\n",
        "#model = load_model('%s/checkpoint_%s_part1'%(folder,modelname))\n",
        "reduce_lr        = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
        "early_stopping   = EarlyStopping(monitor='val_loss', patience=20)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='%s/checkpoint_%s_part1'%(folder,modelname), save_weights_only=False, monitor='val_loss', mode='m\n",
        "in', save_best_only=True)\n",
        "def lr_schedule(epoch):\n",
        "    warmup_epochs  = 10\n",
        "    initial_lr     = 0.005\n",
        "    if epoch < warmup_epochs:\n",
        "        return initial_lr * (warmup_epochs - epoch)\n",
        "    else:\n",
        "        return initial_lr\n",
        "def set_lr(epoch):\n",
        "    return lr_schedule(epoch)\n",
        "\n",
        "def on_reduce_lr(epoch, logs):\n",
        "    model_checkpoint.on_epoch_end(epoch, logs)\n",
        "    model.load_weights('%s/checkpoint_%s_part1'%(folder,modelname))\n",
        "lr_scheduler = LambdaCallback(on_epoch_begin=lambda epoch,logs: \n",
        "                                           setattr(model.optimizer, \"learning_rate\", set_lr(epoch)))\n",
        "opt = Nadam(learning_rate=0.005, epsilon=1e-07)\n",
        "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "with tf.device('/GPU:%s'%(gpu_to_use)):\n",
        "    history = model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), epochs=200, batch_size=1024,callbacks=[reduce_lr,lr_scheduler,early_stopping,m\n",
        "odel_checkpoint])\n",
        "    #history = model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid),initial_epoch=100, epochs=250, batch_size=1024,callbacks=[reduce_lr,lr_schedul\n",
        "er,early_stopping,model_checkpoint])\n",
        "\n",
        "np.save('%s/history_%s_part1.npy'%(folder,modelname), history.history)\n"
      ],
      "metadata": {
        "id": "4hGWQ1vO6HIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}