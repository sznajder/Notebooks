{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/DeepSets_PYTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 10: Deep Sets\n",
        "\n",
        "In this workshop we will be looking at Deep Sets, which appeared in the Week 5 lecture on geometric deep learning.\n",
        "We will use Deep Sets to solve some small (fairly uninteresting) problems - the workshop will be more on how to implement novel architectures in PyTorch, if they are not available as a library."
      ],
      "metadata": {
        "id": "Rwrpnh4XdnrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0flZ1L6cWEL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import itertools\n",
        "import functools\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a DeepSet network?\n",
        "\n",
        "Deep Sets are a particular architecture of neural network which are permutation-invariant with respect to their inputs. Such a network maps $n$ vectors in $\\mathbb{R}^k$ to a single vector in $\\mathbb{R}^r$, in such a way that the output is independent of the input order of the $n$ vectors. Furthermore, the same Deep Set network will be able to accept different numbers $n$ of input vectors."
      ],
      "metadata": {
        "id": "yjtt0yaUeAOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The permutation representation of the symmetric group\n",
        "\n",
        "For now, we will set our input size to $\\mathbb{R}^k = \\mathbb{R}$, so our Deep Set network will be recieving $n$ scalars as input. We treat these scalars as living in the permutation representation $V_n = \\mathbb{R}^n$ of the symmetric group, where each permutation acts by permuting coordinates (i.e. acts as a permutation matrix). There are two canonical subrepresentations: the trivial subrepresentation\n",
        "$$ V_{triv} = \\mathbb{R}(1, 1, \\ldots, 1), $$\n",
        "and the \"standard\" subrepresentation\n",
        "$$ V_{std} = \\{(x_1, \\ldots, x_n) \\in \\mathbb{R}^n \\mid x_1 + \\cdots + x_n = 0\\}.$$\n",
        "Basic representation theory says that if a linear map $f \\colon V_n \\to V_n$ satisfies $f(\\sigma v) = \\sigma f(v)$ for all vectors $v \\in V_n$ and permutations $\\sigma$, the map must be block-diagonal with respect to $V_{triv}$ and $V_{std}$, and furthermore, it must be multiplication by a scalar on each of them. In more concrete terms, the matrix of $f$ must have all equal elements on the on-diagonal and on the off-diagonal:\n",
        "$$ [f] = \\begin{pmatrix} a & b & b & b \\\\ b & a & b & b \\\\ b & b & a & b \\\\ b & b & b & a \\end{pmatrix} = aI + (J - I)b,$$\n",
        "where $I$ is the identity matrix and $J$ is the all-ones matrix.\n",
        "So there is only a two-dimensional space of permutation-equivariant maps $V_n \\to V_n$ (rather than dimension $n^2$ in general). A little thought will show you that the only permutation-invariant biases that we can add as the affine part of a map are multiples of $(1, 1, \\ldots, 1)$.\n",
        "\n",
        "Regular neural networks are built out of layers that look like $\\mathbb{R}^2 \\to \\mathbb{R}^3$ with affine maps (matrices plus a vector) in between. DeepSet networks are instead built out of layers that look like\n",
        "$$ V_n \\oplus V_n \\to V_n \\oplus V_n \\oplus V_n, $$\n",
        "with equivariant maps in between. In this example there would be a 15-dimensional space of affine equivariant maps."
      ],
      "metadata": {
        "id": "pQcnobYLgc9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the DeepSet blocks\n",
        "\n",
        "From a technical standpoint, we want to teach PyTorch about a new kind of module\n",
        "it can use to build neural networks. We will specify the input and output dimensions ($2$ and $3$ above), but _not_ about the $n$ that will be used (since it is variable). This layer needs to then store the $2(2 \\times 3) + 3 = 15$ parameters, and implement the operation which actually performs the linear map (this implementation needs to be able to accept different input cardinalities $n$). The parameters will automatically be available for training, we don't need to provide any code to do that.\n",
        "\n",
        "### Tensor details\n",
        "\n",
        "We will implement a `DeepSetLayer` which takes $V_n^{\\oplus k} \\to V_n^{\\oplus r}$, i.e. it maps $n$ vectors of length $k$ to $n$ vectors of length $r$, for variable length $n$. Let us ignore the batch coordinate for now: we will be transforming a tensor of shape $(k, n)$ to one of shape $(k, r)$. The discussion above shows that the linear map must be of the form\n",
        "$$ \\alpha \\otimes I + \\beta \\otimes J,$$\n",
        "where $I$ is the identity matrix, $J$ is the all-ones matrix, and $\\alpha, \\beta$ are arbitrary $r \\times k$ matrices. The additive bias must be of the form\n",
        "$$ +\\gamma \\otimes (1, 1, \\ldots, 1). $$\n",
        "\n",
        "**Technical note:** One can write the linear map in many different ways. We could for example choose the canonical projectors to the subrepresentations,\n",
        "$$ P_{triv}(x_1, \\ldots, x_n) = \\frac{x_1 + \\cdots + x_n}{n}(1, \\ldots, 1), \\quad P_{std} = I - P_{triv},$$\n",
        "and instead parameterise equivariant maps by $\\alpha \\otimes P_{triv} + \\beta \\otimes P_{std}$. Notice that the definition of $P_{triv}$ and $P_{std}$ depend on $n$ (in the forward pass, there will be a division by $n$). This really has an impact on the network (at least for small networks): for instance using $J$ and $I$ a network is better at calculating the sum of its inputs, wheras using the projectors it is better at calculating the mean.\n",
        "\n",
        "**Technical note 2:** When implementing $\\alpha \\otimes I + \\beta \\otimes J$, we don't actually want to multiply by the matrices $I$ and $J$. There is no point to multiplying by $I$ (just use the input directly), and multiplication by $J$ is better expressed by summing up the coordinates of the vector and then doing tensor broadcasting."
      ],
      "metadata": {
        "id": "0Lv4eckPjmW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rand(shape, low, high):\n",
        "    \"\"\"Tensor of random numbers, uniformly distributed on [low, high].\"\"\"\n",
        "    return torch.rand(shape) * (high - low) + low\n",
        "\n",
        "\n",
        "class DeepSetLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    DeepSetLayer(in_blocks, out_blocks) takes shape (batch, in_blocks, n) to (batch, out_blocks, n).\n",
        "    Each block of n scalars is treated as the S_n permutation representation, and maps between blocks are\n",
        "    S_n-equivariant.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_blocks, out_blocks):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_blocks = in_blocks\n",
        "        self.out_blocks = out_blocks\n",
        "        \n",
        "        # Initialisation tactic copied from nn.Linear in PyTorch\n",
        "        lim = (in_blocks)**-0.5 / 2\n",
        "\n",
        "        # Alpha corresponds to the identity, beta to the all-ones matrix, and gamma to the additive bias.\n",
        "        self.alpha = torch.nn.Parameter(data=rand((out_blocks, in_blocks), -lim, lim))\n",
        "        self.beta = torch.nn.Parameter(data=rand((out_blocks, in_blocks), -lim, lim))\n",
        "        self.gamma = torch.nn.Parameter(data=rand((out_blocks), -lim, lim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch, in_blocks, n)\n",
        "        return (\n",
        "            torch.einsum('...jz, ij -> ...iz', x, self.alpha)\n",
        "            + torch.einsum('...jz, ij -> ...iz', x.sum(axis=-1)[..., None], self.beta)\n",
        "            + self.gamma[..., None]\n",
        "        )\n",
        "\n",
        "\n",
        "# Test if we got our tensor nonsense right. For a batch size of 6, and n = 7,\n",
        "# a DeepSetLayer(2, 3) should map shape (6, 2, 7) to (6, 3, 7).\n",
        "DeepSetLayer(2, 3)(torch.rand((6, 2, 7))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFY6ckbd_NN",
        "outputId": "b42de82c-01cc-4589-93b9-822093412271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting an invariant\n",
        "\n",
        "We eventually want an output of a single vector in $\\mathbb{R}^r$, rather than $n$ vectors in $\\mathbb{R}^r$, which can be either used directly, or fed into a standard neural network. The `DeepSetSum` block implements this, by summing along the last tensor dimension. (Representation-theoretically, we are projecting to a direct sum of trivial representations, which turns our equivariant network into an invariant network).\n"
      ],
      "metadata": {
        "id": "7xokxv3lodA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepSetSum(nn.Module):\n",
        "    \"\"\"\n",
        "    DeepSetSum(blocks) takes a deep set layer of shape (batch, blocks, n) to a regular layer\n",
        "    of shape (batch, blocks) by projecting to the trivial representation and then extracting\n",
        "    a coordinate, eg\n",
        "        (1, 2, 3, 4) => project to trivial => (2.5, 2.5, 2.5, 2.5) => extract component => 2.5\n",
        "    \"\"\"\n",
        "    def __init__(self, blocks):\n",
        "        super().__init__()\n",
        "        \n",
        "        lim = (blocks)**-0.5 / 2\n",
        "        self.weight = torch.nn.Parameter(data=rand(blocks, -lim, lim))\n",
        "        self.bias = torch.nn.Parameter(data=rand(blocks, -lim, lim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.sum(dim=-1) * self.weight + self.bias\n",
        "  \n",
        "# Check shapes. A DeepSetSum(3) should take shape (batch, 3, n) to (batch, 3)\n",
        "DeepSetSum(3)(torch.rand((6, 3, 7))).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddu2DoS2n2Vz",
        "outputId": "8aa19d7f-5e9c-4919-c94f-3092fa313c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Find the sum of a set of scalars.\n",
        "\n",
        "Given a multiset of scalars in the range $[-1, 1]$, output their sum.\n",
        "This is (of course) a rather boring task for a neural network, but it serves as somewhere we can contrast the Deep Set architecture with a more standard approach.\n",
        "\n",
        "We're using the following utility function to count the parameters (as in the number of trainable scalars) in a model:"
      ],
      "metadata": {
        "id": "EEwvD5H6paJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Return the number of trainable parameters of a model (the total number of scalars).\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "fvQmOWQ4qIbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A standard neural network\n",
        "\n",
        "With a standard neural network, we will have to fix our number of scalars $n$ in advance. Other than that, it is business as usual. We just generate test data on-the-fly for this simple problem."
      ],
      "metadata": {
        "id": "_5Qc2mTqpsc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of numbers to add (needs to be fixed for the standard network!)\n",
        "input_size = 4\n",
        "\n",
        "# Model structure.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 1),\n",
        ")\n",
        "print(f\"Model has {count_parameters(model)} parameters\")\n",
        "\n",
        "# Learning rate and loss function.\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "# Samples each epoch, and batches for gradient descent.\n",
        "epochs = 30\n",
        "sample_size = 10_000\n",
        "batch_size = 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHJvf7_Vpq_n",
        "outputId": "c69479d7-420f-4d6e-c297-4593a4ff6d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 321 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "recorded_loss = torch.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    samples = rand((sample_size, input_size), -1, 1)\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for i in range(0, sample_size, batch_size):\n",
        "        batch_in = samples[i:i+batch_size]\n",
        "        batch_target = batch_in.sum(dim=-1)[:, None]\n",
        "        \n",
        "        optimiser.zero_grad()\n",
        "        result = model(batch_in)\n",
        "        loss = loss_function(result, batch_target)\n",
        "        total_loss += float(loss)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    \n",
        "    recorded_loss[epoch] = total_loss\n",
        "      \n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(recorded_loss)\n",
        "plt.show()\n",
        "print(f\"Final loss is {recorded_loss[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "SNrOv2mYpWmy",
        "outputId": "cb2a50c5-b642-4fff-d4fa-793c3135c811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss is 20.522085189819336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The network is doing fairly well, and by examining some explicit outputs we can see that it's almost reached invariance:"
      ],
      "metadata": {
        "id": "AsqJDyEoqWTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = torch.tensor([\n",
        "    [0, 1, 0, 1],\n",
        "    [1, 0, 1, 0],\n",
        "    [-1, 1, -1, 1],\n",
        "    [1, -1, 1, -1],\n",
        "    [0, 0, 0, 0],\n",
        "], dtype=torch.float)\n",
        "\n",
        "with torch.no_grad():\n",
        "    results = model(examples)\n",
        "\n",
        "results.numpy().round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrrNBvyXqRBT",
        "outputId": "5db2894f-fec6-4ced-e7ba-478931ceafa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.  ],\n",
              "       [ 2.01],\n",
              "       [ 0.01],\n",
              "       [-0.03],\n",
              "       [-0.01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Deep Set network"
      ],
      "metadata": {
        "id": "PYMpd5GVrEqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model structure. Note that we don't specify the input size anywhere!!!\n",
        "model = nn.Sequential(\n",
        "    DeepSetLayer(1, 20),\n",
        "    nn.ReLU(),\n",
        "    DeepSetLayer(20, 10),\n",
        "    nn.ReLU(),\n",
        "    DeepSetLayer(10, 1),\n",
        "    DeepSetSum(1),\n",
        ")\n",
        "print(f\"Model has {count_parameters(model)} parameters\")\n",
        "\n",
        "# Learning rate and loss function.\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "# Samples each epoch, and batches for gradient descent.\n",
        "epochs = 30\n",
        "sample_size = 10_000\n",
        "batch_size = 16\n",
        "\n",
        "# Input set sizes to use during training\n",
        "input_sizes = [3, 4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlYe4A_Gqxbk",
        "outputId": "87eef488-4d35-465a-8061-0c21ad831371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 493 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "data = []\n",
        "for n_inputs in input_sizes:\n",
        "    samples = torch.from_numpy(np.random.random_sample((sample_size//len(input_sizes), 1, n_inputs))).float()\n",
        "    for i in range(0, samples.shape[0], batch_size):\n",
        "        data += [samples[i:i+batch_size]]\n",
        "\n",
        "\n",
        "recorded_loss = torch.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    random.shuffle(data)\n",
        "    for samples in data:\n",
        "        batch_in = samples\n",
        "        batch_target = batch_in.sum(-1)\n",
        "        \n",
        "        optimiser.zero_grad()\n",
        "        result = model(batch_in)\n",
        "        loss = loss_function(result, batch_target)\n",
        "        total_loss += float(loss)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    \n",
        "    recorded_loss[epoch] = total_loss\n",
        "    # print(f\"Epoch {epoch}, loss {total_loss}\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(recorded_loss)\n",
        "#ax.set_ylim([0, 1])\n",
        "plt.show()\n",
        "print(f\"Final loss is {recorded_loss[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "nSeIvkQZrdyA",
        "outputId": "dbe0797b-4287-4ec5-f354-264d53cb400b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss is 22.87281608581543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = torch.tensor([\n",
        "    [0, 1, 0, 1],\n",
        "    [1, 0, 1, 0],\n",
        "    [-1, 1, -1, 1],\n",
        "    [1, -1, 1, -1],\n",
        "    [0, 0, 0, 0],\n",
        "], dtype=torch.float).reshape(-1, 1, 4)\n",
        "\n",
        "with torch.no_grad():\n",
        "    results = model(examples)\n",
        "\n",
        "results.numpy().round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E30hC9WsGbZ",
        "outputId": "4c79f541-3587-48c5-b994-ac0af67b7bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.99],\n",
              "       [ 1.99],\n",
              "       [-0.39],\n",
              "       [-0.39],\n",
              "       [-0.55]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ONPayBTvsLNT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}