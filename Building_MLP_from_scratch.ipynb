{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/Building_MLP_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_r0qgqRgA3"
      },
      "source": [
        "## Copied from \n",
        "https://github.com/aayushmnit/Deep_learning_explorations/tree/master/1_MLP_from_scratch\n",
        "\n",
        "In this notebook, we are going to build a neural network(multilayer perceptron) using numpy and successfully train it to recognize digits in the image. Deep learning is a vast topic, but we got to start somewhere, so let's start with the very basics of a neural network which is Multilayer Perceptron. \n",
        "\n",
        "## What is a neural network?\n",
        "A neural network is a type of machine learning model which is inspired by our neurons in the brain where many neurons are connected with many other neurons to translate an input to an output (simple right?). Mostly we can look at any machine learning model and think of it as a function which takes an input and produces the desired output; it's the same with a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-hl9m_5RgA-"
      },
      "source": [
        "## What is a Multi layer perceptron?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irYCMIqKRgA_"
      },
      "source": [
        "Multi-layer perceptron is a type of network where multiple layers of a group of perceptron are stacked together to make a model. Before we jump into the concept of a layer and multiple perceptrons, let's start with the building block of this network which is a perceptron. Think of perceptron/neuron as a linear model which takes multiple inputs and produce an output. In our case perceptron is a linear model which takes a bunch of inputs multiply them with weights and add a bias term to generate an output.<br/>\n",
        "<center>$Z= \\vec{w} \\cdot X + {b}$<center> <br/>\n",
        "<img src=\"https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/perceptron.png\" align='center'>\n",
        "<center>*Fig 1: Perceptron image*<center> <br/>\n",
        "<div align='right'>*Image credit=https://commons.wikimedia.org/wiki/File:Perceptron.png/*</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T17:51:22.143963-05:00",
          "start_time": "2018-06-03T17:51:22.136946Z"
        },
        "id": "_i6i_ar4RgA_"
      },
      "source": [
        "Now, if we stack a bunch of these perceptrons together, it becomes a hidden layer which is also known as a Dense layer in modern deep learning terminology. <br/>\n",
        "<center>**Dense layer,** $f(X)=W \\cdot X + \\vec{b}$</center> <br/>\n",
        "*Note that bias term is now a vector and W is a weight matrix* <br/>\n",
        "<img src=\"https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/single_layer_mlp.png\" align ='center'>\n",
        "<center>***Fig: Single dense layer perceptron network***<center> <br/>\n",
        "<div align='right'>*Image credit=http://www.texample.net/tikz/examples/neural-network/*</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T17:59:18.675024-05:00",
          "start_time": "2018-06-03T17:59:18.668498Z"
        },
        "id": "3OxYitaYRgBA"
      },
      "source": [
        "Now we understand dense layer let's add a bunch of them, and that network becomes a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:01:09.397157-05:00",
          "start_time": "2018-06-03T18:01:09.392611Z"
        },
        "id": "mlTDI-RMRgBA"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/aayushmnit/Deep_learning_explorations/master/1_MLP_from_scratch/multi_layer_mlp.png\" align ='center'>\n",
        "<center>***Fig: Multi layer perceptron network***<center> <br/>\n",
        "<div align='right'> *Image credit=http://pubs.sciepub.com/ajmm/3/3/1/figure/2*</div>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T17:40:31.942655-05:00",
          "start_time": "2018-06-03T17:40:31.937674Z"
        },
        "id": "BZ-GS3ryRgBB"
      },
      "source": [
        "If you have noticed our dense layer, only have linear functions, and any combination of linear function only results in the linear output. As we want our MLP to be flexible and learn non-linear decision boundaries, we also need to introduce non-linearity into the network. We achieve the task of introducing non-linearity by adding activation function. There are various kinds of activation function which can be used, but we will be implementing Rectified Linear Units(ReLu) which is one of the popular activation function. ReLU function is a simple function which is zero for any input value below zero and the same value for values greater than zero. <br/>\n",
        "<center>**ReLU function ** $f(X) = max(0,X)$</center>\n",
        "<br/>\n",
        "Now, we understand dense layer and also understand the purpose of activation function, the only thing left is training the network. For training a neural network we need to have a loss function and every layer should have a **feed-forward loop** and **backpropagation loop**. Feedforward loop takes an input and generates output for making a prediction and backpropagation loop helps in training the model by adjusting weights in the layer to lower the output loss. In backpropagation, the weight update is done by using backpropagated gradients using the chain rule and optimized using an optimization algorithm. In our case, we will be using SGD(stochastic gradient descent). If you don't understand the concept of gradient weight updates and SGD, I recommend you to watch [week 1 of Machine learning by Andrew NG lectures](https://www.coursera.org/ml)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-05-15T23:16:05.888459-05:00",
          "start_time": "2018-05-15T23:16:04.070039Z"
        },
        "id": "UGxpgQzSRgBC"
      },
      "source": [
        "So, to summarize a neural network needs few building blocks\n",
        "\n",
        "- Dense layer - a fully-connected layer, $f(X)=W \\cdot X + \\vec{b}$\n",
        "- ReLU layer (or any other activation function to introduce non-linearity)\n",
        "- Loss function - (crossentropy in case of multi-class classification problem)\n",
        "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n",
        "\n",
        "Let's approach them one at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYMNL4ziRgBD"
      },
      "source": [
        "## Coding Starts here --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:29:49.723482-05:00",
          "start_time": "2018-06-03T18:29:49.718413Z"
        },
        "id": "PGYa3ItTRgBE"
      },
      "source": [
        "Let's start by importing some libraires required for creating our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:30:07.531098-05:00",
          "start_time": "2018-06-03T18:30:07.527088Z"
        },
        "id": "azQuJ0ceRgBF"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np ## For numerical python\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NryZdf9pRgBH"
      },
      "source": [
        "Every layer will have a forward pass and backpass implementation. Let's create a main class layer which can do a forward pass *.forward()* and Backward pass *.backward().*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T19:03:20.951163-05:00",
          "start_time": "2018-06-03T19:03:20.941168Z"
        },
        "id": "r0uLZfmKRgBH"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "    \n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "    \n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here we can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        # A dummy layer does nothing\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        # A dummy layer just returns whatever it gets as input.\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "        \n",
        "        To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n",
        "        \n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "        \n",
        "        Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "        \n",
        "        If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
        "        num_units = input.shape[1]\n",
        "        \n",
        "        d_layer_d_input = np.eye(num_units)\n",
        "        \n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3vdKEW3RgBI"
      },
      "source": [
        "### Nonlinearity ReLU layer\n",
        "\n",
        "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T19:03:31.531589-05:00",
          "start_time": "2018-06-03T19:03:31.525072Z"
        },
        "id": "6gOukMbZRgBI"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        relu_forward = np.maximum(0,input)\n",
        "        return relu_forward\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEPBQnNnRgBJ"
      },
      "source": [
        "### Dense layer\n",
        "\n",
        "Now let's build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\n",
        "\n",
        "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
        "$$f(X)= W \\cdot X + \\vec b $$\n",
        "\n",
        "Where \n",
        "* X is an object-feature matrix of shape [batch_size, num_features],\n",
        "* W is a weight matrix [num_features, num_outputs] \n",
        "* and b is a vector of num_outputs biases.\n",
        "\n",
        "Both W and b are initialized during layer creation and updated each time backward is called. Note that we are using **Xavier initialization** which is a trick to train our model to converge faster [read more](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization). Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T19:04:12.511355-05:00",
          "start_time": "2018-06-03T19:04:12.492305Z"
        },
        "id": "RxL7foqeRgBJ"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.random.normal(loc=0.0, \n",
        "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
        "                                        size = (input_units,output_units))\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        return np.dot(input,self.weights) + self.biases\n",
        "    \n",
        "    def backward(self,input,grad_output):\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = np.dot(input.T, grad_output)\n",
        "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
        "        \n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        \n",
        "        # Here we perform a stochastic gradient descent step. \n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        \n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzrk-zGYRgBJ"
      },
      "source": [
        "### The loss function\n",
        "\n",
        "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\n",
        "\n",
        "If we write down the expression for crossentropy as a function of softmax logits (a), you'll see:\n",
        "\n",
        "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
        "\n",
        "If we take a closer look, we'll see that it can be rewritten as:\n",
        "\n",
        "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
        "\n",
        "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
        "* Better numerical stability\n",
        "* Easier to get derivative right\n",
        "* Marginally faster to compute\n",
        "\n",
        "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T19:04:47.947883-05:00",
          "start_time": "2018-06-03T19:04:47.939333Z"
        },
        "id": "SR5Ew0OURgBK"
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "    \n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    \n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxO_tit_RgBK"
      },
      "source": [
        "### Full network\n",
        "\n",
        "Now let's combine what we've just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example. Fortunately, Keras already have it in the numpy array format, so let's import it!."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:44:01.717629-05:00",
          "start_time": "2018-06-03T18:43:20.450314Z"
        },
        "id": "9HaHNMFRRgBN",
        "outputId": "cceee873-9251-4686-a40f-82dab20ad1c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAF1CAYAAADx1LGMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x1c929c0fa58>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def load_dataset(flatten=False):\n",
        "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # normalize x\n",
        "    X_train = X_train.astype(float) / 255.\n",
        "    X_test = X_test.astype(float) / 255.\n",
        "\n",
        "    # we reserve the last 10000 training examples for validation\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
        "\n",
        "## Let's look at some example\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls_qid6oRgBO"
      },
      "source": [
        "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:44:01.727147-05:00",
          "start_time": "2018-06-03T18:44:01.719126Z"
        },
        "id": "lnkOQKBMRgBO"
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Dense(X_train.shape[1],100))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(100,200))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(200,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:46:26.631468-05:00",
          "start_time": "2018-06-03T18:46:26.608900Z"
        },
        "id": "AwUmqJALRgBP"
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer. \n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    # Looping through each layer\n",
        "    for l in network:\n",
        "        activations.append(l.forward(input))\n",
        "        # Updating input to last layer output\n",
        "        input = activations[-1]\n",
        "    \n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network,X):\n",
        "    \"\"\"\n",
        "    Compute network predictions. Returning indices of largest Logit probability\n",
        "    \"\"\"\n",
        "    logits = forward(network,X)[-1]\n",
        "    return logits.argmax(axis=-1)\n",
        "\n",
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train our network on a given batch of X and y.\n",
        "    We first need to run forward to get all layer activations.\n",
        "    Then we can run layer.backward going from last to first layer.\n",
        "    After we have called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "    \n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "    \n",
        "    # Propagate gradients through the network\n",
        "    # Reverse propogation as this is backprop\n",
        "    for layer_index in range(len(network))[::-1]:\n",
        "        layer = network[layer_index]\n",
        "        \n",
        "        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n",
        "        \n",
        "    return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DcMoTHvRgBP"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "We split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:47:45.846651-05:00",
          "start_time": "2018-06-03T18:47:45.316719Z"
        },
        "id": "r4eY5zalRgBP"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:47:51.387863-05:00",
          "start_time": "2018-06-03T18:47:51.384341Z"
        },
        "id": "8X3129RkRgBP"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-06-03T18:49:20.383230-05:00",
          "start_time": "2018-06-03T18:47:53.604185Z"
        },
        "id": "vXYF_o8PRgBQ",
        "outputId": "c5a48d14-8f29-4d6e-f596-32ea69781708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24\n",
            "Train accuracy: 1.0\n",
            "Val accuracy: 0.9809\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/A3nCPpTI68gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x1c933114eb8>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for epoch in range(25):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
        "        train(network,x_batch,y_batch)\n",
        "    \n",
        "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
        "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
        "    \n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE6ZRamERgBQ"
      },
      "source": [
        "As we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:py35]",
      "language": "python",
      "name": "conda-env-py35-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "name": "Building_MLP_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}