{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNckwmw7NiA9169yzKf/t3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/HEPAnalysis_DataframeBased_TFKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIPaFCU9AiFX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') #this stops matplotlib trying to use Xwindows backend when running remotely\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras import callbacks\n",
        "\n",
        "from MlClasses.MlData import MlData\n",
        "from MlClasses.Dnn import Dnn\n",
        "\n",
        "#===== Define some useful variables =====\n",
        "\n",
        "makePlots=True\n",
        "doClassification=False\n",
        "doRegression=True\n",
        "\n",
        "output='exampleOut' # an output directory (then make it if it doesn't exist)\n",
        "if not os.path.exists(output): os.makedirs(output)\n",
        "\n",
        "#class to stop training of dnns early\n",
        "earlyStopping = callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=2)\n",
        "\n",
        "lumi=30. #luminosity in /fb\n",
        "expectedBkgd=844000.*8.2e-4*lumi #cross section of ttbar sample in fb times efficiency measured by Marco\n",
        "\n",
        "#ttbar background and stop (900,100) \n",
        "# df = pd.read_pickle('/nfs/dust/cms/user/elwoodad/dlNonCms/hepML/dfs/combined.pkl')\n",
        "# expectedSignal=17.6*0.059*lumi #cross section of stop sample in fb times efficiency measured by Marco\n",
        "\n",
        "#ttbar background and stop (600,400) \n",
        "dfFull = pd.read_pickle('/nfs/dust/cms/user/elwoodad/dlNonCms/hepML/dfs/combinedleonid.pkl')\n",
        "expectedSignal=228.195*0.14*lumi \n",
        "\n",
        "#===== Load the data from a pickle file (choose one of the two below) =====\n",
        "\n",
        "#Pick a subset of events to limit size for messing about\n",
        "#be careful to pick randomly as the first half are signal and the second half background\n",
        "dfFull = dfFull.sample(100000,random_state=42)\n",
        "\n",
        "#Look at the variables in the trees:\n",
        "\n",
        "print 'The keys are:'\n",
        "print dfFull.keys()\n",
        "\n",
        "#Define and select a subset of the variables:\n",
        "\n",
        "subset=['signal', #1 for signal and 0 for background\n",
        "        'HT','MET', #energy sums\n",
        "        'MT','MT2W', #topological variables\n",
        "        'n_jet','n_bjet', #jet and b-tag multiplicities\n",
        "        'sel_lep_pt0','sel_lep_eta0','sel_lep_phi0', #lepton 4-vector\n",
        "        'selJet_phi0','selJet_pt0','selJet_eta0','selJet_m0',# lead jet 4-vector\n",
        "        'selJet_phi1','selJet_pt1','selJet_eta1','selJet_m1',# second jet 4-vector\n",
        "        'selJet_phi2','selJet_pt2','selJet_eta2','selJet_m2']# third jet 4-vector\n",
        "\n",
        "df=dfFull[subset]\n",
        "\n",
        "print 'The reduced keys are:'\n",
        "print df.keys()\n",
        "\n",
        "if makePlots:\n",
        "    #===== Make a couple of plots: =====\n",
        "\n",
        "    #Calculate the weights for each event and add them to the dataframe\n",
        "    signalWeight = expectedSignal/(df.signal==1).sum() #divide expected events by number in dataframe\n",
        "    bkgdWeight   = expectedBkgd/(df.signal==0).sum()\n",
        "\n",
        "    #Add a weights column with the correct weights for background and signal\n",
        "    df['weight'] = df['signal']*signalWeight+(1-df['signal'])*bkgdWeight\n",
        "\n",
        "    #Choose some variables to plot and loop over them\n",
        "    varsToPlot = ['HT','MT','MET','sel_lep_pt0','selJet_pt0']\n",
        "\n",
        "    for v in varsToPlot:\n",
        "\n",
        "        print 'Plotting',v\n",
        "        maxRange=max(df[v])\n",
        "        #Plot the signal and background but stacked on top of each other\n",
        "        plt.hist([df[df.signal==0][v],df[df.signal==1][v]], #Signal and background input\n",
        "                label=['background','signal'],\n",
        "                bins=50, range=[0.,maxRange], \n",
        "                stacked=True, color = ['g','r'],\n",
        "                weights=[df[df.signal==0]['weight'],df[df.signal==1]['weight']]) #supply the weights\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel(v)\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(output,'hist_'+v+'.pdf')) #save the histogram\n",
        "        plt.clf() #Clear it for the next one\n",
        "\n",
        "    df = df.drop('weight',axis=1) #drop the weight to stop inference from it as truth variable\n",
        "\n",
        "if doClassification:\n",
        "\n",
        "    #=============================================================\n",
        "    #===== Make a simple network to carry out classification =====\n",
        "    #=============================================================\n",
        "\n",
        "    print 'Running classification'\n",
        "\n",
        "    # here I make use of the hepML framework with keras\n",
        "    # aim is to correctly classify signal or background events\n",
        "\n",
        "    #===== Prepare the data =====\n",
        "    # use an MlData class to wrap and manipulate the data with easy functions\n",
        "\n",
        "    print 'Preparing data'\n",
        "\n",
        "    mlDataC = MlData(df,'signal') #insert the dataframe and tell it what the truth variable is\n",
        "    mlDataC.split(evalSize=0.0,testSize=0.3) #Split into train and test sets, leave out evaluation set for now\n",
        "\n",
        "    #Now decide whether we want to standardise the dataset\n",
        "    #it is worth seeing what happens to training with and without this option\n",
        "    #(this must be done after the split to avoid information leakage)\n",
        "    mlDataC.standardise()\n",
        "\n",
        "    #===== Setup and run the network  =====\n",
        "\n",
        "    print 'Setting up network'\n",
        "\n",
        "    dnnC = Dnn(mlDataC,os.path.join(output,'classification')) #give it the data and an output directory for plots\n",
        "\n",
        "    #build a 2 hidden layer model with 50 neurons each layer\n",
        "    #Note: if the number of neurons is a float it treats it as a proportion of the input\n",
        "    # loss is binary cross entropy and one sigmoid neuron is used for output\n",
        "    dnnC.setup(hiddenLayers=[20,20],dropOut=None,l2Regularization=None,loss='binary_crossentropy') \n",
        "\n",
        "    #fit the defined network with the data passed to it\n",
        "    #define an early stopping if the loss stops decreasing after 2 epochs\n",
        "    print 'Fitting'\n",
        "    dnnC.fit(epochs=100,batch_size=128,callbacks=[earlyStopping])\n",
        "\n",
        "    #now produce some diagnostics to see how it went\n",
        "    print 'Making diagnostics'\n",
        "    dnnC.diagnostics() #generic diagnostics, ROC curves etc\n",
        "    # hep specific plots including sensitivity estimates with a flat systematic etc: \n",
        "    print '\\nMaking HEP plots'\n",
        "    dnnC.makeHepPlots(expectedSignal,expectedBkgd,systematics=[0.2],makeHistograms=False)\n",
        "\n",
        "if doRegression:\n",
        "\n",
        "    #=========================================================\n",
        "    #===== Make a simple network to carry out regression =====\n",
        "    #=========================================================\n",
        "\n",
        "    #now we've seen a classification example, try a similar thing with regression\n",
        "    #try to predict a higher level variable from the low level inputs\n",
        "\n",
        "    print 'Running regression'\n",
        "\n",
        "    print 'Preparing data'\n",
        "\n",
        "    #Just pick the 4-vectors to train on\n",
        "    subset = ['HT']\n",
        "    for k in dfFull.keys():\n",
        "        for v in ['selJet','sel_lep']:\n",
        "            if ' '+v in ' '+k: subset.append(k)\n",
        "\n",
        "    print 'Using subset',subset\n",
        "    df=dfFull[subset]\n",
        "\n",
        "    df=df.fillna(0) #NaNs in the input cause problems\n",
        "\n",
        "    #insert the dataframe without the background class and the variable for regression\n",
        "    mlDataR = MlData(df,'HT') \n",
        "\n",
        "    mlDataR.split(evalSize=0.0,testSize=0.3) #Split into train and test sets, leave out evaluation set for now\n",
        "\n",
        "    #Now decide whether we want to standardise the dataset\n",
        "    #it is worth seeing what happens to training with and without this option\n",
        "    #(this must be done after the split to avoid information leakage)\n",
        "    #mlDataR.standardise() #find this causes problems with regression\n",
        "\n",
        "    print 'Setting up network'\n",
        "\n",
        "    dnnR=Dnn(mlDataR,os.path.join(output,'regression'),doRegression=True)\n",
        "\n",
        "    #here sets up with mean squared error and a linear output neuron\n",
        "    dnnR.setup(hiddenLayers=[20,20],dropOut=None,l2Regularization=None)#,loss='mean_squared_error')\n",
        "\n",
        "    print 'Fitting'\n",
        "    dnnR.fit(epochs=100,batch_size=128,callbacks = [earlyStopping])\n",
        "\n",
        "    print 'Making diagnostics'\n",
        "    dnnR.diagnostics() #make regression specific diagnostics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform GRID Search\n"
      ],
      "metadata": {
        "id": "EsqjPvssBM2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from dfConvert import convertTree\n",
        "\n",
        "from keras import callbacks\n",
        "\n",
        "from pandasPlotting.Plotter import Plotter\n",
        "from pandasPlotting.dfFunctions import expandArrays\n",
        "from pandasPlotting.dtFunctions import featureImportance\n",
        "\n",
        "from MlClasses.MlData import MlData\n",
        "from MlClasses.Bdt import Bdt\n",
        "from MlClasses.Dnn import Dnn\n",
        "from MlClasses.ComparePerformances import ComparePerformances\n",
        "\n",
        "from MlFunctions.DnnFunctions import significanceLoss,significanceLossInvert,significanceLoss2Invert,significanceLossInvertSqrt,significanceFull,asimovSignificanceLoss,asimovSignificanceLossInvert,asimovSignificanceFull,truePositive,falsePositive\n",
        "\n",
        "from linearAlgebraFunctions import gram,addGramToFlatDF\n",
        "from root_numpy import rec2array\n",
        "\n",
        "timingFile = open('testPlots/timings.txt','w')\n",
        "#Callback to move onto the next batch after stopping\n",
        "earlyStopping = callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=2)\n",
        "\n",
        "nInputFiles=100\n",
        "limitSize=None#100000 #Make this an integer N_events if you want to limit input\n",
        "\n",
        "#Use these to calculate the significance when it's used for training\n",
        "#Taken from https://twiki.cern.ch/twiki/bin/view/CMS/SummerStudent2017#SUSY\n",
        "# (dependent on batch size)\n",
        "lumi=30. #luminosity in /fb\n",
        "expectedSignal=17.6*0.059*lumi #cross section of stop sample in fb times efficiency measured by Marco\n",
        "#expectedSignal=228.195*0.14*lumi #leonid's number\n",
        "expectedBkgd=844000.*8.2e-4*lumi #cross section of ttbar sample in fb times efficiency measured by Marco\n",
        "systematic=0.1 #systematic for the asimov signficance\n",
        "\n",
        "makeDfs=False\n",
        "saveDfs=True #Save the dataframes if they're remade\n",
        "#appendInputName='leonid'\n",
        "appendInputName=''\n",
        "\n",
        "makePlots=False\n",
        "\n",
        "prepareInputs=False\n",
        "addGramMatrix=False\n",
        "\n",
        "#ML options\n",
        "plotFeatureImportances=False\n",
        "doBDT=False\n",
        "doDNN=True\n",
        "doCrossVal=False\n",
        "makeLearningCurve=False\n",
        "doGridSearch=False #if this is true do a grid search, if not use the configs\n",
        "\n",
        "doRegression=True\n",
        "regressionVars=['MT2W']#,'HT']\n",
        "\n",
        "makeHistograms=False\n",
        "\n",
        "normalLoss=True\n",
        "sigLoss=False\n",
        "sigLossInvert=True\n",
        "sigLoss2Invert=False\n",
        "sigLossInvertSqrt=False\n",
        "asimovSigLoss=False\n",
        "asimovSigLossInvert=False\n",
        "asimovSigLossBothInvert=True\n",
        "asimovSigLossSysts=[0.01,0.05,0.1,0.2,0.3,0.4,0.5]\n",
        "crossEntropyFirst=False\n",
        "variableBatchSigLossInvert=False\n",
        "\n",
        "#If not doing the grid search\n",
        "dnnConfigs={\n",
        "    #'dnn':{'epochs':100,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "      # 'dnn_batch128':{'epochs':200,'batch_size':128,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "      #  'dnn_batch2048':{'epochs':200,'batch_size':2048,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "      'dnn_batch4096':{'epochs':200,'batch_size':4096,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "     #'dnn_batch1024':{'epochs':200,'batch_size':1024,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "    # 'dnn_batch8192':{'epochs':200,'batch_size':8192,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "    # 'dnn2l':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0,1.0]},\n",
        "    # 'dnn3l':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0,1.0,1.0]},\n",
        "    # 'dnn3l_batch1024':{'epochs':40,'batch_size':1024,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0,1.0,1.0]},\n",
        "    # 'dnn5l':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[1.0,1.0,1.0,1.0,1.0]},\n",
        "    # 'dnn_2p0n':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[2.0]},\n",
        "    # 'dnn2l_2p0n':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[2.0,2.0]},\n",
        "    # 'dnn3l_2p0n':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    # 'dnn4l_2p0n':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0,2.0]},\n",
        "    # 'dnn5l_2p0n':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0,2.0,2.0]},\n",
        "\n",
        "    # 'dnn_l2Reg0p01':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[1.0]},\n",
        "    # 'dnn2l_l2Reg0p01':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[1.0,1.0]},\n",
        "    # 'dnn3l_l2Reg0p01':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[1.0,1.0,1.0]},\n",
        "    # 'dnn5l_l2Reg0p01':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[1.0,1.0,1.0,1.0,1.0]},\n",
        "    # 'dnn2l_2p0n_l2Reg0p01':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[2.0,2.0]},\n",
        "    # 'dnn3l_2p0n_l2Reg0p01':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    # 'dnn4l_2p0n_l2Reg0p01':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[2.0,2.0,2.0,2.0]},\n",
        "    # 'dnn5l_2p0n_l2Reg0p01':{'epochs':50,'batch_size':32,'dropOut':None,'l2Regularization':0.1,'hiddenLayers':[2.0,2.0,2.0,2.0,2.0]},\n",
        "\n",
        "    # 'dnndo0p5':{'epochs':10,'batch_size':32,'dropOut':0.5,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "    # 'dnn2ldo0p5':{'epochs':10,'batch_size':32,'dropOut':0.5,'l2Regularization':None,'hiddenLayers':[1.0,0.5]},\n",
        "    # 'dnndo0p2':{'epochs':30,'batch_size':32,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0]},\n",
        "    # 'dnn2ldo0p2':{'epochs':30,'batch_size':32,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0,1.0]},\n",
        "    # 'dnn3ldo0p2':{'epochs':30,'batch_size':32,'dropOut':0.2,'l2Regularization':None,'hiddenLayers':[1.0,1.0,1.0]},\n",
        "    # 'dnnSmall':{'epochs':20,'batch_size':32,'dropOut':None,'l2Regularization':None,'l2Regularization':None,'hiddenLayers':[0.3]},\n",
        "    # 'dnn2lSmall':{'epochs':20,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[0.66,0.3]},\n",
        "    # 'dnn3lSmall':{'epochs':40,'batch_size':32,'dropOut':None,'l2Regularization':None,'hiddenLayers':[0.66,0.5,0.3]},\n",
        "\n",
        "    #Bests\n",
        "    #4 vector\n",
        "    # 'dnn3l_2p0n_do0p25':{'epochs':40,'batch_size':32,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    # 'dnn3l_2p0n_do0p25_batch128':{'epochs':40,'batch_size':128,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    # 'dnn3l_2p0n_do0p25_batch1024':{'epochs':40,'batch_size':1024,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    # 'dnn3l_2p0n_do0p25_batch2048':{'epochs':40,'batch_size':2048,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    #'dnn3l_2p0nIdo0p25_batch4096':{'epochs':200,'batch_size':4096,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    #'dnn3l_2p0n_do0p25_batch8192':{'epochs':40,'batch_size':8192,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0]},\n",
        "    #'dnn5l_1p0n_do0p25':{'epochs':200,'batch_size':4096,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[1.0,1.0,1.0,1.0,1.0]},\n",
        "    #'dnn4l_2p0n_do0p25':{'epochs':40,'batch_size':32,'dropOut':0.25,'l2Regularization':None,'hiddenLayers':[2.0,2.0,2.0,2.0]},\n",
        "    #'dnn2lWide':{'epochs':30,'batch_size':32,'dropOut':0.25,'hiddenLayers':[2.0,2.0]},\n",
        "        }\n",
        "\n",
        "#If doing the grid search\n",
        "def hiddenLayerGrid(nLayers,nNodes):\n",
        "    hlg=[]\n",
        "    for nn in nNodes:\n",
        "        for nl in nLayers:\n",
        "            hlg.append([nn for x in range(nl)])\n",
        "        pass\n",
        "    return hlg\n",
        "\n",
        "dnnGridParams = dict(\n",
        "        mlp__epochs=[10,20,50],\n",
        "        mlp__batch_size=[32,64], \n",
        "        mlp__hiddenLayers=hiddenLayerGrid([1,2,3,4,5],[2.0,1.0,0.5]),\n",
        "        mlp__dropOut=[None,0.25,0.5],\n",
        "        # mlp__activation=['relu','sigmoid','tanh'],\n",
        "        # mlp__optimizer=['adam','sgd','rmsprop'],\n",
        "        ## NOT IMPLEMENTED YET:\n",
        "        # mlp__learningRate=[0.5,1.0], \n",
        "        # mlp__weightConstraint=[1.0,3.0,5.0]\n",
        "        )\n",
        "\n",
        "bdtGridParams = dict(\n",
        "        base_estimator__max_depth=[3,5],\n",
        "        base_estimator__min_samples_leaf=[0.05,0.2],\n",
        "        n_estimators=[400,800]\n",
        "        )\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    #############################################################\n",
        "    #Either make the dataframes fresh from the trees or just read them in\n",
        "    if makeDfs:\n",
        "        print \"Making DataFrames\"\n",
        "\n",
        "        signalFile = []#'/nfs/dust/cms/group/susy-desy/marco/training_sample_new/stop_sample_0.root'\n",
        "        bkgdFile = []#'/nfs/dust/cms/group/susy-desy/marco/training_sample_new/top_sample_0.root'\n",
        "\n",
        "        for i in range(nInputFiles):\n",
        "            signalFile.append(' /nfs/dust/cms/group/susy-desy/marco/leonid/stop_600_400/stop_samples_'+str(i)+'.root')\n",
        "            bkgdFile.append('/nfs/dust/cms/group/susy-desy/marco/training_sample_new/top_sample_'+str(i)+'.root')\n",
        "\n",
        "        signal = convertTree(signalFile,signal=True,passFilePath=True,tlVectors = ['selJet','sel_lep'])\n",
        "        bkgd = convertTree(bkgdFile,signal=False,passFilePath=True,tlVectors = ['selJet','sel_lep'])\n",
        "\n",
        "        # #Expand the variables to 1D\n",
        "        signal = expandArrays(signal) \n",
        "        bkgd = expandArrays(bkgd) \n",
        "\n",
        "        if saveDfs:\n",
        "            print 'Saving the dataframes'\n",
        "            # Save the dfs?\n",
        "            if not os.path.exists('dfs'): os.makedirs('dfs')\n",
        "            print 'signal size:',len(signal)\n",
        "            signal.to_pickle('dfs/signal'+appendInputName+'.pkl')\n",
        "            print 'bkgd size:',len(bkgd)\n",
        "            bkgd.to_pickle('dfs/bkgd'+appendInputName+'.pkl')\n",
        "    else:\n",
        "        print \"Loading DataFrames\" \n",
        "\n",
        "        signal = pd.read_pickle('dfs/signal'+appendInputName+'.pkl')\n",
        "        bkgd = pd.read_pickle('dfs/bkgd'+appendInputName+'.pkl')\n",
        "\n",
        "    if makePlots:\n",
        "        print \"Making plots\" \n",
        "        #Skip out excessive jet info\n",
        "        exceptions=[] \n",
        "        for k in signal.keys():\n",
        "            if 'selJet' in k or '_x' in k or '_y' in k or '_z' in k:\n",
        "                exceptions.append(k)\n",
        "        signalPlotter = Plotter(signal.copy(),'testPlots/signal',exceptions=exceptions)\n",
        "        bkgdPlotter = Plotter(bkgd.copy(),'testPlots/bkgd',exceptions=exceptions)\n",
        "\n",
        "        signalPlotter.plotAllHists1D(withErrors=True)\n",
        "        signalPlotter.correlations()\n",
        "        bkgdPlotter.plotAllHists1D(withErrors=True)\n",
        "        bkgdPlotter.correlations()\n",
        "        pass\n",
        "\n",
        "    #############################################################\n",
        "    #Carry out the organisation of the inputs or read them in if it's already done\n",
        "    if prepareInputs:\n",
        "        print 'Preparing inputs'\n",
        "\n",
        "        # Put the data in a format for the machine learning: \n",
        "        # combine signal and background with an extra column indicating which it is\n",
        "\n",
        "        signal['signal'] = 1\n",
        "        bkgd['signal'] = 0\n",
        "\n",
        "        combined = pd.concat([signal,bkgd])\n",
        "\n",
        "        #Now add the relevant variables to the DFs (make gram matrix)\n",
        "\n",
        "        #Make a matrix of J+L x J+L where J is the number of jets and L is the number of leptons\n",
        "        #Store it as a numpy matrix in the dataframe \n",
        "        \n",
        "\n",
        "        #METHOD 1:\n",
        "        # Store as a matrix in the numpy array\n",
        "        # It's better for it to be flat for machine learning... so using method 2\n",
        "\n",
        "        #Use function that takes (4x) arrays of objects (for E,px,py,pz) and returns matrix\n",
        "        #Must store it as an array of arrays as 2D numpy objects can't be stored in pandas\n",
        "        \n",
        "        #print 'm',signal['selJet_m'][0]+signal['sel_lep_m'][0]\n",
        "        # signal['gram'] = signal.apply(lambda row: gram(row['sel_lep_e']+[row['MET']]+row['selJet_e'],\\\n",
        "        #     row['sel_lep_px']+[row['MET']*math.cos(row['METPhi'])]+row['selJet_px'],\\\n",
        "        #     row['sel_lep_py']+[row['MET']*math.sin(row['METPhi'])]+row['selJet_py'],\\\n",
        "        #     row['sel_lep_pz']+[0]+row['selJet_pz']),axis=1)\n",
        "        #\n",
        "        # bkgd['gram'] = bkgd.apply(lambda row: gram(row['sel_lep_e']+[row['MET']]+row['selJet_e'],\\\n",
        "        #     row['sel_lep_px']+[row['MET']*math.cos(row['METPhi'])]+row['selJet_px'],\\\n",
        "        #     row['sel_lep_py']+[row['MET']*math.sin(row['METPhi'])]+row['selJet_py'],\\\n",
        "        #     row['sel_lep_pz']+[0]+row['selJet_pz']),axis=1)\n",
        "\n",
        "        #METHOD 2:\n",
        "        #Put MET into the same format as the other objects\n",
        "        if addGramMatrix:\n",
        "            print 'Producing GRAM matrix'\n",
        "            combined['MET_e']=combined['MET']\n",
        "            combined.drop('MET',axis=1)#Drop the duplicate\n",
        "            combined['MET_px']=combined['MET']*np.cos(combined['METPhi'])\n",
        "            combined['MET_py']=combined['MET']*np.sin(combined['METPhi'])\n",
        "            combined['MET_pz']=0\n",
        "            nSelLep = 0\n",
        "            nSelJet = 0\n",
        "            for k in combined.keys():\n",
        "                if 'sel_lep_px' in k: nSelLep+=1\n",
        "                if 'selJet_px' in k: nSelJet+=1\n",
        "            addGramToFlatDF(combined,single=['MET'],multi=[['sel_lep',nSelLep],['selJet',nSelJet]])\n",
        "\n",
        "        if saveDfs:\n",
        "            print 'Saving prepared files'\n",
        "            combined.to_pickle('dfs/combined'+appendInputName+'.pkl')\n",
        "\n",
        "    else:\n",
        "        print 'Reading prepared files'\n",
        "        combined = pd.read_pickle('dfs/combined'+appendInputName+'.pkl')\n",
        "\n",
        "    #Now carry out machine learning (with some algo specific diagnostics)\n",
        "    #Choose the variables to train on\n",
        "\n",
        "    chosenVars = {\n",
        "            #Just the gram matrix, with or without b info\n",
        "            # 'gram':['signal','gram'],\n",
        "            #\n",
        "            # 'gramBL':['signal','gram','selJetB','lep_type'],\n",
        "            #\n",
        "            # 'gramMT':['signal','gram','MT'],\n",
        "            #\n",
        "            # 'gramMT2W':['signal','gram','MT2W'],\n",
        "            #\n",
        "            # 'gramHT':['signal','gram','HT'],\n",
        "            #\n",
        "            # #The 4 vectors only\n",
        "            # 'fourVector':['signal',\n",
        "            # 'sel_lep_pt','sel_lep_eta','sel_lep_phi','sel_lep_m',\n",
        "            # 'selJet_phi','selJet_pt','selJet_eta','selJet_m','MET'],\n",
        "            #\n",
        "            # 'fourVectorBL':['signal','lep_type','selJetB',\n",
        "            # 'sel_lep_pt','sel_lep_eta','sel_lep_phi','sel_lep_m',\n",
        "            # 'selJet_phi','selJet_pt','selJet_eta','selJet_m','MET'],\n",
        "            #\n",
        "            # 'fourVectorMT':['signal',\n",
        "            # 'sel_lep_pt','sel_lep_eta','sel_lep_phi','sel_lep_m',\n",
        "            # 'selJet_phi','selJet_pt','selJet_eta','selJet_m','MET','MT'],\n",
        "            #\n",
        "            # 'fourVectorMT2W':['signal',\n",
        "            # 'sel_lep_pt','sel_lep_eta','sel_lep_phi','sel_lep_m',\n",
        "            # 'selJet_phi','selJet_pt','selJet_eta','selJet_m','MET','MT2W'],\n",
        "            #\n",
        "            # 'fourVectorHT':['signal',\n",
        "            # 'sel_lep_pt','sel_lep_eta','sel_lep_phi','sel_lep_m',\n",
        "            # 'selJet_phi','selJet_pt','selJet_eta','selJet_m','MET','HT'],\n",
        "            #\n",
        "            # #A vanilla analysis with HL variables and lead 3 jets\n",
        "            'vanilla':['signal','HT','MET','MT','MT2W','n_jet',\n",
        "            'n_bjet','sel_lep_pt0','sel_lep_eta0','sel_lep_phi0',\n",
        "            'selJet_phi0','selJet_pt0','selJet_eta0','selJet_m0',\n",
        "            'selJet_phi1','selJet_pt1','selJet_eta1','selJet_m1',\n",
        "            'selJet_phi2','selJet_pt2','selJet_eta2','selJet_m2'],\n",
        "\n",
        "            }\n",
        "\n",
        "    trainedModels={}\n",
        "\n",
        "    for varSetName,varSet in chosenVars.iteritems():\n",
        "\n",
        "        print ''\n",
        "        print '==========================='\n",
        "        print 'Analysing var set '+varSetName\n",
        "        print '==========================='\n",
        "\n",
        "        #Pick out the expanded arrays\n",
        "        columnsInDataFrame = []\n",
        "        for k in combined.keys():\n",
        "            for v in varSet:\n",
        "                #Little trick to ensure only the start of the string is checked\n",
        "                if varSetName is 'vanilla':\n",
        "                    if ' '+v+' ' in ' '+k+' ': columnsInDataFrame.append(k)\n",
        "                elif ' '+v in ' '+k: columnsInDataFrame.append(k)\n",
        "\n",
        "\n",
        "        #Select just the features we're interested in\n",
        "        #For now setting NaNs to 0 for compatibility\n",
        "        combinedToRun = combined[columnsInDataFrame].copy()\n",
        "        combinedToRun.fillna(0,inplace=True)\n",
        "\n",
        "        # print columnsInDataFrame\n",
        "        # print exit()\n",
        "\n",
        "        #############################################################\n",
        "        #Now everything is ready can start the machine learning\n",
        "\n",
        "        if plotFeatureImportances:\n",
        "            print 'Making feature importances'\n",
        "            #Find the feature importance with a random forest classifier\n",
        "            featureImportance(combinedToRun,'signal','testPlots/mlPlots/'+varSetName+'/featureImportance')\n",
        "\n",
        "        print 'Splitting up data'\n",
        "\n",
        "        mlData = MlData(combinedToRun,'signal')\n",
        "\n",
        "        #Now split pseudorandomly into training and testing\n",
        "        #Split the development set into training and testing\n",
        "        #(forgetting about evaluation for now)\n",
        "\n",
        "        mlData.prepare(evalSize=0.0,testSize=0.3,limitSize=limitSize)\n",
        "\n",
        "        if doBDT:\n",
        "\n",
        "            if doGridSearch:\n",
        "                print 'Running BDT grid search'\n",
        "                bdt = Bdt(mlData,'testPlots/mlPlots/'+varSetName+'/bdtGridSearch')\n",
        "                bdt.setup()\n",
        "                bdt.gridSearch(param_grid=bdtGridParams,kfolds=3,n_jobs=4)\n",
        "\n",
        "            elif not doRegression:\n",
        "                #Start with a BDT from sklearn (ala TMVA)\n",
        "                print 'Defining and fitting BDT'\n",
        "                bdt = Bdt(mlData,'testPlots/mlPlots/'+varSetName+'/bdt')\n",
        "                bdt.setup()\n",
        "                bdt.fit()\n",
        "                if doCrossVal:\n",
        "                    print ' > Carrying out cross validation'\n",
        "                    bdt.crossValidation(kfolds=5)\n",
        "                if makeLearningCurve:\n",
        "                    print ' > Making learning curves'\n",
        "                    bdt.learningCurve(kfolds=5,n_jobs=3)\n",
        "\n",
        "                #and carry out a diagnostic of the results\n",
        "                print ' > Producing diagnostics'\n",
        "                bdt.diagnostics()\n",
        "\n",
        "                trainedModels[varSetName+'_bdt']=bdt\n",
        "\n",
        "        if doDNN:\n",
        "\n",
        "            if doGridSearch:\n",
        "                print 'Running DNN grid search'\n",
        "                dnn = Dnn(mlData,'testPlots/mlPlots/'+varSetName+'/dnnGridSearch')\n",
        "                dnn.setup()\n",
        "                dnn.gridSearch(param_grid=dnnGridParams,kfolds=3,epochs=20,batch_size=32,n_jobs=4)\n",
        "\n",
        "            if doRegression:\n",
        "\n",
        "                for name,config in dnnConfigs.iteritems():\n",
        "\n",
        "                    for regressionVar in regressionVars:\n",
        "\n",
        "                        if regressionVar not in varSet: continue\n",
        "\n",
        "                        #Drop unconverged events for MT2\n",
        "                        if regressionVar is 'MT2W': \n",
        "                            toRunRegression=combinedToRun[combinedToRun.MT2W!=999.0]\n",
        "                        else:\n",
        "                            toRunRegression=combinedToRun\n",
        "                        \n",
        "\n",
        "                        mlDataRegression = MlData(toRunRegression.drop('signal'),regressionVar)\n",
        "                        mlDataRegression.prepare(evalSize=0.0,testSize=0.2,limitSize=limitSize,standardise=False)\n",
        "\n",
        "                        print 'Defining and fitting DNN',name,'Regression',regressionVar\n",
        "                        dnn = Dnn(mlDataRegression,'testPlots/mlPlots/regression/'+varSetName+'/'+name,doRegression=True)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
        "                        dnn.save()\n",
        "\n",
        "                        if makeLearningCurve:\n",
        "                            print ' > Making learning curves'\n",
        "                            dnn.learningCurve(kfolds=3,n_jobs=1,scoring='neg_mean_squared_error')\n",
        "\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics()\n",
        "\n",
        "            \n",
        "            else:\n",
        "                #Now lets move on to a deep neural net \n",
        "                for name,config in dnnConfigs.iteritems():\n",
        "\n",
        "                    if normalLoss:\n",
        "                        print 'Defining and fitting DNN',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                    ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=128,callbacks=[earlyStopping])\n",
        "                        dnn.save()\n",
        "                        if doCrossVal:\n",
        "                            print ' > Carrying out cross validation'\n",
        "                            dnn.crossValidation(kfolds=5,epochs=config['epochs'],batch_size=config['batch_size'])\n",
        "                        if makeLearningCurve:\n",
        "                            print ' > Making learning curves'\n",
        "                            dnn.learningCurve(kfolds=5,n_jobs=1)\n",
        "\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.explainPredictions()\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_'+name]=dnn\n",
        "\n",
        "                    if sigLoss:\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        timingFile.write('\\nTiming normal batch, with DNN '+name)\n",
        "                        t0=time.time()\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/sigLoss/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLoss(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.explainPredictions()\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        t1=time.time()\n",
        "                        timingFile.write(': '+str(t1-t0)+' s\\n')\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_sigLoss_'+name]=dnn\n",
        "\n",
        "                    if sigLossInvert:\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/sigLossInvert/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLossInvert(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_sigLossInvert_'+name]=dnn\n",
        "\n",
        "                    if sigLoss2Invert:\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/sigLoss2Invert/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLoss2Invert(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_sigLoss2Invert_'+name]=dnn\n",
        "\n",
        "\n",
        "                    if sigLossInvertSqrt:\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/sigLossInvertSqrt/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLossInvertSqrt(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,asimovSigLossSysts,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_sigLossInvertSqrt_'+name]=dnn\n",
        "                    \n",
        "\n",
        "                    if asimovSigLossInvert:\n",
        "\n",
        "                        #First set up a model that trains on the sig loss\n",
        "                        print 'Defining and fitting DNN with inverted asimov significance loss function',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/asimovSigLossInvert/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLoss(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    asimovSignificanceLossInvert(expectedSignal,expectedBkgd,systematic),asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),\n",
        "                                    significanceFull(expectedSignal,expectedBkgd),truePositive,falsePositive\n",
        "                                ])\n",
        "\n",
        "                        dnn.fit(epochs=5,batch_size=config['batch_size'])\n",
        "                        dnn.diagnostics(batchSize=8192,subDir='pretraining')\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,systematic,makeHistograms=makeHistograms,subDir='pretraining')\n",
        "\n",
        "                        #Now recompile the model with a different loss and train further\n",
        "                        dnn.recompileModel(asimovSignificanceLossInvert(expectedSignal,expectedBkgd,systematic))\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.explainPredictions()\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,[systematic],makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_asimovSigLossInvert_'+name]=dnn\n",
        "\n",
        "                    if asimovSigLossBothInvert:\n",
        "\n",
        "                        for chosenSyst in asimovSigLossSysts:\n",
        "\n",
        "                            systName = str(chosenSyst).replace('.','p')\n",
        "\n",
        "                            #First set up a model that trains on the sig loss\n",
        "                            print 'Defining and fitting DNN with inverted asimov significance loss function',name\n",
        "                            dnn = Dnn(mlData,'testPlots/mlPlots/asimovSigLossBothInvertSyst'+systName+'/'+varSetName+'/'+name)\n",
        "                            dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                    loss=significanceLoss2Invert(expectedSignal,expectedBkgd),\n",
        "                                    extraMetrics=[\n",
        "                                        asimovSignificanceLossInvert(expectedSignal,expectedBkgd,chosenSyst),asimovSignificanceFull(expectedSignal,expectedBkgd,chosenSyst),\n",
        "                                        significanceFull(expectedSignal,expectedBkgd),truePositive,falsePositive\n",
        "                                    ])\n",
        "\n",
        "                            dnn.fit(epochs=5,batch_size=config['batch_size'])\n",
        "                            dnn.diagnostics(batchSize=8192,subDir='pretraining')\n",
        "                            dnn.makeHepPlots(expectedSignal,expectedBkgd,[chosenSyst],makeHistograms=makeHistograms,subDir='pretraining')\n",
        "\n",
        "                            #Now recompile the model with a different loss and train further\n",
        "                            dnn.recompileModel(asimovSignificanceLossInvert(expectedSignal,expectedBkgd,chosenSyst))\n",
        "                            dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'],callbacks=[earlyStopping])\n",
        "                            dnn.save()\n",
        "                            print ' > Producing diagnostics'\n",
        "                            dnn.diagnostics(batchSize=8192)\n",
        "                            dnn.makeHepPlots(expectedSignal,expectedBkgd,[chosenSyst],makeHistograms=makeHistograms)\n",
        "\n",
        "                            trainedModels[varSetName+'_asimovSigLossBothInvert_'+name+systName]=dnn\n",
        "\n",
        "                    if asimovSigLoss:\n",
        "\n",
        "                        print 'Defining and fitting DNN with asimov significance loss function',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/asimovSigLoss/'+varSetName+'/'+name)\n",
        "\n",
        "                        #First set up a model that trains on the sig loss\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLoss(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    asimovSignificanceLoss(expectedSignal,expectedBkgd,systematic),asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),\n",
        "                                    significanceFull(expectedSignal,expectedBkgd),truePositive,falsePositive\n",
        "                                ])\n",
        "\n",
        "                        dnn.fit(epochs=5,batch_size=config['batch_size'])\n",
        "                        dnn.diagnostics(batchSize=8192,subDir='pretraining')\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,systematic,makeHistograms=makeHistograms,subDir='pretraining')\n",
        "\n",
        "                        #Now recompile the model with a different loss and train further\n",
        "                        dnn.recompileModel(asimovSignificanceLoss(expectedSignal,expectedBkgd,systematic))\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=8192)\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,[systematic],makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_asimovSigLoss_'+name]=dnn\n",
        "\n",
        "                    if crossEntropyFirst:\n",
        "\n",
        "                        #First train and fit with the cross entropy loss function\n",
        "                        print 'Defining and fitting DNN',name\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/crossEntropyFirst/'+varSetName+'/'+name)\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                    ])\n",
        "                        dnn.fit(epochs=config['epochs'],batch_size=128)\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=128,subDir='pretraining')\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,systematic,makeHistograms=makeHistograms,subDir='pretraining')\n",
        "\n",
        "                        #Now cut away the obvious background, weight up the background events and retrain\n",
        "\n",
        "                        #Remove all x entropy background\n",
        "                        dataToPredict = combinedToRun.drop('signal',axis=1)\n",
        "                        dataToPredict = dataToPredict.loc[:,~dataToPredict.columns.duplicated()]\n",
        "                        # print '======'\n",
        "                        # print dataToPredict.columns\n",
        "                        # print dnn.data.X_train.columns\n",
        "                        dataToPredict = dnn.data.scaler.transform(dataToPredict.as_matrix())\n",
        "                        toRunXEntropyFirst = combinedToRun[dnn.model.predict(dataToPredict)>0.5]\n",
        "\n",
        "                        #weight up the background based on the inbalance\n",
        "                        nSignal = len(toRunXEntropyFirst[toRunXEntropyFirst.signal==1])\n",
        "                        nBkgd = len(toRunXEntropyFirst[toRunXEntropyFirst.signal==0])\n",
        "                        upWeight = float(nSignal)/nBkgd\n",
        "\n",
        "                        def fabricateWeight(signal,weight):\n",
        "                            if signal==0: return weight\n",
        "                            else: return 1.0\n",
        "\n",
        "                        weights = toRunXEntropyFirst.apply(lambda row: fabricateWeight(row.signal,upWeight),axis=1) \n",
        "\n",
        "                        mlDataXEntropyFirst = MlData(toRunXEntropyFirst,'signal',weights=weights.as_matrix())\n",
        "                        mlDataXEntropyFirst.prepare(evalSize=0.0,testSize=0.3,limitSize=limitSize)\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        dnn2 = Dnn(mlDataXEntropyFirst,'testPlots/mlPlots/crossEntropyFirst/'+varSetName+'/'+name)\n",
        "                        dnn2.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLoss(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "                        dnn2.fit(epochs=config['epochs'],batch_size=config['batch_size'])\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn2.diagnostics(batchSize=8192)\n",
        "\n",
        "                        #Now need to make the HEP plots with the full dataset by passing a custom prediction that includes the decision from dnn2\n",
        "                        # In this case we take xEntropy Score > 0.5 and then the sig loss output\n",
        "                        # i.e. if xEntropyScore < 0.5 return 0, else return sig loss output\n",
        "\n",
        "                        #save the prediction from dnn\n",
        "                        #firstPred = dnn.testPrediction()\n",
        "                        firstPred = dnn.model.predict_classes(dnn.data.X_test.as_matrix())\n",
        "                        #unscale the data in dnn\n",
        "                        dataForPred2=dnn.data.scaler.inverse_transform(dnn.data.X_test)\n",
        "                        #rescale the data to dnn2\n",
        "                        dataForPred2=dnn2.data.scaler.transform(dataForPred2)\n",
        "                        #predict dnn2\n",
        "                        secondPred = dnn2.model.predict(dataForPred2)\n",
        "                        #combine the predictions\n",
        "                        finalPred = firstPred*secondPred\n",
        "\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,systematic,makeHistograms=makeHistograms,customPrediction=finalPred)\n",
        "\n",
        "                    if variableBatchSigLossInvert:\n",
        "\n",
        "                        print 'Defining and fitting DNN with significance loss function',name\n",
        "                        timingFile.write('\\nTiming variable batch, with DNN '+name)\n",
        "                        t0=time.time()\n",
        "\n",
        "                        dnn = Dnn(mlData,'testPlots/mlPlots/variableBatchSigLossInvert/'+varSetName+'/'+name)\n",
        "\n",
        "                        dnn.setup(hiddenLayers=config['hiddenLayers'],dropOut=config['dropOut'],l2Regularization=config['l2Regularization'],\n",
        "                                loss=significanceLossInvert(expectedSignal,expectedBkgd),\n",
        "                                extraMetrics=[\n",
        "                                    significanceLoss(expectedSignal,expectedBkgd),significanceFull(expectedSignal,expectedBkgd),\n",
        "                                    asimovSignificanceFull(expectedSignal,expectedBkgd,systematic),truePositive,falsePositive\n",
        "                                ])\n",
        "\n",
        "\n",
        "                        for batch_size in [128,512,1024,2048,4096,8192,16384]:\n",
        "                            print '\\nRunning with batch size '+str(batch_size)\n",
        "                            dnn.fit(epochs=100,batch_size=batch_size,callbacks=[earlyStopping])\n",
        "                            dnn.diagnostics(batchSize=4096,subDir='batchSize'+str(batch_size))\n",
        "\n",
        "                        dnn.save()\n",
        "                        print ' > Producing diagnostics'\n",
        "                        dnn.diagnostics(batchSize=4096)\n",
        "                        t1=time.time()\n",
        "                        timingFile.write(': '+str(t1-t0)+' s\\n')\n",
        "                        dnn.makeHepPlots(expectedSignal,expectedBkgd,systematic,makeHistograms=makeHistograms)\n",
        "\n",
        "                        trainedModels[varSetName+'_variableBatchSigLossInvert_'+name]=dnn\n",
        "        \n",
        "\n",
        "                pass\n",
        "\n",
        "    pass # end of variable set loop\n",
        "\n",
        "    #Compare all the results\n",
        "    if not doGridSearch and not doRegression:\n",
        "\n",
        "        # #Now compare all the different versions\n",
        "        compareMl = ComparePerformances(trainedModels,output='testPlots/mlPlots/comparisons')\n",
        "\n",
        "        compareMl.compareRoc(append='_all')\n",
        "        compareMl.rankMethods()\n",
        "        #\n",
        "\n",
        "        compareMl.compareRoc(['gram_dnn','gram_dnn3l_2p0n_do0p25','gram_dnn5l_1p0n_do0p25','gram_dnn4l_2p0n_do0p25'],append='_gramOnly')\n",
        "        compareMl.compareRoc(['fourVector_dnn','fourVector_dnn3l_2p0n_do0p25','fourVector_dnn5l_1p0n_do0p25','fourVector_dnn4l_2p0n_do0p25'],append='_fourVectorOnly')\n",
        "        compareMl.compareRoc(['fourVector_dnn','fourVector_dnn3l_2p0n_do0p25','fourVector_dnn5l_1p0n_do0p25','fourVector_dnn4l_2p0n_do0p25'],append='_fourVectorOnly')\n",
        "\n",
        "\n",
        "        # #compareMl.compareRoc(['gram_dnn2l','gramMT_dnn2l','gramHT_dnn2l','gramMT2W_dnn2l','gramBL_dnn2l'],append='_gramOnlyDNN2l')\n",
        "        # compareMl.compareRoc(['gram_dnn2ldo0p2','gramMT_dnn2ldo0p2','gramHT_dnn2ldo0p2','gramMT2W_dnn2ldo0p2','gramBL_dnn2ldo0p2'],append='_gramOnlyDNN2ldo0p2')\n",
        "        # compareMl.compareRoc(['gram_dnn3ldo0p2','gramMT_dnn3ldo0p2','gramHT_dnn3ldo0p2','gramMT2W_dnn3ldo0p2','gramBL_dnn3ldo0p2'],append='_gramOnlyDNN3ldo0p2')\n",
        "        # compareMl.compareRoc(['gram_bdt','gramMT_bdt','gramHT_bdt','gramMT2W_bdt','gramBL_bdt'], append='_gramOnlyBDT')\n",
        "        #\n",
        "        # compareMl.compareRoc(['fourVector_dnn','fourVectorMT_dnn','fourVectorHT_dnn','fourVectorMT2W_dnn','fourVectorBL_dnn'],append='_fourVectorOnlyDNN')\n",
        "        # #compareMl.compareRoc(['fourVector_dnn2l','fourVectorMT_dnn2l','fourVectorHT_dnn2l','fourVectorMT2W_dnn2l','fourVectorBL_dnn2l'],append='_fourVectorOnlyDNN2l')\n",
        "        # compareMl.compareRoc(['fourVector_dnn2ldo0p2','fourVectorMT_dnn2ldo0p2','fourVectorHT_dnn2ldo0p2','fourVectorMT2W_dnn2ldo0p2','fourVectorBL_dnn2ldo0p2'],append='_fourVectorOnlyDNN2ldo0p2')\n",
        "        # compareMl.compareRoc(['fourVector_dnn3ldo0p2','fourVectorMT_dnn3ldo0p2','fourVectorHT_dnn3ldo0p2','fourVectorMT2W_dnn3ldo0p2','fourVectorBL_dnn3ldo0p2'],append='_fourVectorOnlyDNN3ldo0p2')\n",
        "        # compareMl.compareRoc(['fourVector_bdt','fourVectorMT_bdt','fourVectorHT_bdt','fourVectorMT2W_bdt','fourVectorBL_bdt'], append='_fourVectorOnlyBDT')\n",
        "        #\n",
        "        compareMl.compareRoc(['gram_dnn5l_1p0n_do0p25','gram_bdt',\n",
        "            'fourVector_dnn3l_2p0n_do0p25','fourVector_bdt',\n",
        "            'vanilla_dnn3l_2p0n_do0p25','vanilla_dnn5l_1p0n_do0p25','vanilla_bdt'],\n",
        "            append='_vanillaComparisons')\n",
        "        #\n",
        "\n",
        "        #DNN study\n",
        "        # compareMl = ComparePerformances(trainedModels,output='testPlots/mlPlots/dnnStudy')\n",
        "        # compareMl.compareRoc(append='_all')\n",
        "        # compareMl.rankMethods()\n",
        "        #\n",
        "        # #BDT study\n",
        "        # compareMl = ComparePerformances(trainedModels,output='testPlots/mlPlots/bdtStudy')\n",
        "        # compareMl.compareRoc(append='_all')\n",
        "        # compareMl.rankMethods()\n",
        "\n",
        "        pass"
      ],
      "metadata": {
        "id": "JWceDHaLBR91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}