{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch]",
      "language": "python",
      "name": "conda-env-pytorch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "FwdMuonGCNHitClassifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "jxFBp9ERwT5W"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/FwdMuonGCNHitClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vclyua3cwT4r"
      },
      "source": [
        "# Graph Neural Network for HIT classification\n",
        "\n",
        "### Based on:\n",
        "\n",
        "https://github.com/jmduarte/heptrkx-gnn-tracking/blob/master/README.md\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/GCN_Toy2D.ipynb\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/GCN_Seg_Toy2D.ipynb\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/GCN_Toy2D_dev.ipynb\n",
        "\n",
        "\\\\\n",
        "\n",
        "There are a variety of ways to construct neural network graph operations which resemble convolutions. Here are a few possible \"kernels\":\n",
        "\n",
        "1)  $\\tilde{X} = \\sigma(X W_0 + D X W_1 + A X W_2)$, where A is the adjacency matrix containing edge weights, D is the diagonal degree matrix, and the $W$s are learnable weight matrices. \\\\\n",
        "\n",
        "2)  $\\tilde{X} = \\sigma(D^{-1}A X W)$, where $D^{-1}A$ is the normalized adjacency matrix. \\\\\n",
        "\n",
        "3) $\\tilde{X} = \\sigma(\\hat{D}^{-{1 \\over 2}} \\hat{A} \\hat{D}^{-{1 \\over 2}} X W)$, where $\\hat{A} = A + I$ and $\\hat{D}^{-{1 \\over 2}} \\hat{A} \\hat{D}^{-{1 \\over 2}}$ is the symmetrically normalized adjacency matrix.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Normalizing the adjacency\n",
        "A typical thing to do is $D^{-1}A$, where $D$ is the diagonal degree matrix. This amounts to normalizing all edges coming into a node so they sum to 1. Or in other words it normalizes the adjacency matrix elements by $A_{ij}= \\frac{1}{\\sqrt{d_i d_j}}$ , which are the the degrees of nodes $i$ and $j$ \n",
        "\n",
        "The approach of Kipf and Welling is $\\hat{D}^{-{1 \\over 2}} \\hat{A} \\hat{D}^{-{1 \\over 2}}$, where $\\hat{A} = A + I$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43wElIHTwT4v"
      },
      "source": [
        "# System imports\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import os\n",
        "import sys\n",
        "import multiprocessing as mp\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Externals\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix, find\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.spatial import cKDTree\n",
        "import sklearn.metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Torch imports\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "import torch as t\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive on remote Colab machine\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "sys.path.append('gdrive/My Drive/Colab Notebooks')\n",
        "\n",
        "!ls 'gdrive/My Drive/Colab Notebooks/Data'\n",
        "data_dir = 'gdrive/My Drive/Colab Notebooks/Data'\n",
        "\n",
        "# Unzip the graphs into local /tmp\n",
        "!apt install unzip\n",
        "!unzip -o 'gdrive/My Drive/Colab Notebooks/Data/graphs.zip'  -d '/tmp'\n",
        "\n",
        "# Input and Output files and events to read\n",
        "events_start=0\n",
        "events_end=1000\n",
        "\n",
        "\n",
        "# Local imports\n",
        "#from estimator import Estimator\n",
        "#from acts import process_hits_files, select_barrel_hits\n",
        "\n",
        "#%matplotlib notebook\n",
        "#%matplotlib ipympl\n",
        "%matplotlib inline\n",
        "\n",
        "# Training concurrency\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "cuda = False\n",
        "\n",
        "if cuda:\n",
        "    np_to_torch = lambda x, volatile=False: (\n",
        "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile).cuda())\n",
        "else:\n",
        "    np_to_torch = lambda x, volatile=False: (\n",
        "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile))\n",
        "\n",
        "torch_to_np = lambda x: x.cpu().data.numpy()\n",
        "\n",
        "# Graph is a namedtuple of (X, Ri, Ro, y) for convenience\n",
        "#feature_names = ['vh_sim_r', 'vh_sim_phi', 'vh_sim_z']\n",
        "#feature_scale = np.array([1000., 360. / 6., 1000.])\n",
        "\n",
        "feature_names = ['vh_sim_r', 'vh_sim_phi', 'vh_sim_z', 'vh_bend']\n",
        "feature_scale = np.array([1000., 360. / 6., 1000.,1000.])\n",
        "\n",
        "Graph = namedtuple('Graph', ['X', 'Ri', 'Ro', 'y_hits', 'y_segs'])\n",
        "\n",
        "# Sparse graph uses the indices for the Ri, Ro matrices\n",
        "SparseGraph = namedtuple('SparseGraph',['X', 'Ri_rows', 'Ri_cols', 'Ro_rows', 'Ro_cols', 'y_hits'])\n",
        "\n",
        "def graph_to_sparse(graph):\n",
        "    Ri_rows, Ri_cols = graph.Ri.nonzero()\n",
        "    Ro_rows, Ro_cols = graph.Ro.nonzero()\n",
        "    return dict(X=graph.X, y=graph.y,\n",
        "                Ri_rows=Ri_rows, Ri_cols=Ri_cols,\n",
        "                Ro_rows=Ro_rows, Ro_cols=Ro_cols)\n",
        "\n",
        "def sparse_to_graph(X, Ri_rows, Ri_cols, Ro_rows, Ro_cols, y, dtype=np.uint8):\n",
        "    n_nodes, n_edges = X.shape[0], Ri_rows.shape[0]\n",
        "    Ri = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
        "    Ro = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
        "    Ri[Ri_rows, Ri_cols] = 1\n",
        "    Ro[Ro_rows, Ro_cols] = 1\n",
        "    return Graph(X, Ri, Ro, y_hits)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysbAgzhQyOz"
      },
      "source": [
        "## PYTORCH module implementing a Graph Convolutional Network ( GCN )\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/GCN_Seg_Toy2D.ipynb\n",
        "\n",
        "\\\\\n",
        "\n",
        "Convolutional layer definition :\n",
        "$ h_i^{(l+1)} = \\sigma\\left(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{(l)}W^{(l)}\\right) $\n",
        "\n",
        ", where $\\mathcal{N}(i)$ is the neighboring nodes set of node $i$ and $c_{ij}$ is equal to the product of the square root of node degrees $\\sqrt{|\\mathcal{N}(i)|}$ and  $\\sigma$  is the activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQJbkxBQ7Lv"
      },
      "source": [
        "#def repeat_module(module, x):\n",
        "#    \"\"\"Applies module on last axis, repeating over others\"\"\"\n",
        "#    return module(x.view(-1, x.size(-1))).view(x.size()[:-1] + (-1,))\n",
        "\n",
        "\n",
        "class GraphConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal graph convolution layer.\n",
        "    \n",
        "    This module takes an input tensor of node features X and some form\n",
        "    of adjacency matrix A and applies a linear transformation of the form\n",
        "        A * X * W + b\n",
        "    where W and b are weights and biases, respectively.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GraphConv, self).__init__()\n",
        "        # We implement the patch operator as a linear module\n",
        "        self.neighbor_mod = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        # Print NN parameters and values\n",
        "        print(\"self.neighbor_mod:\")\n",
        "        for name, tensor in self.neighbor_mod.named_parameters():\n",
        "          print(\"{:6s}  -  {}\".format(name, tensor.shape))\n",
        "    \n",
        "    def forward(self, x, a):\n",
        "        ax = torch.matmul(a, x)\n",
        "        h = self.neighbor_mod(ax)\n",
        "        return h \n",
        "\n",
        "\n",
        "class GraphConvSelfInt(nn.Module):\n",
        "    \"\"\"\n",
        "    A graph convolution layer with separate explicit self-interaction terms.\n",
        "    \n",
        "    This module takes an input tensor of node features X and adjancency\n",
        "    matrix A and applies a linear transformation of the form\n",
        "        X*W1 + A*X*W2 + b\n",
        "    where (W1, W2) and b are learned weights and biases.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GraphConvSelfInt, self).__init__()\n",
        "        self.node_mod = nn.Linear(input_dim, output_dim)\n",
        "        self.neighbor_mod = nn.Linear(input_dim, output_dim, bias=False)\n",
        "\n",
        "        # Print NN parameters and values\n",
        "        print(\"self.node_mod:\")\n",
        "        for name, tensor in self.node_mod.named_parameters():\n",
        "          print(\"{:6s}  -  {}\".format(name, tensor.shape))\n",
        "        print(\"self.neighbor_mod:\")\n",
        "        for name, tensor in self.neighbor_mod.named_parameters():\n",
        "          print(\"{:6s}  -  {}\".format(name, tensor.shape))\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        ax = torch.matmul(a, x)\n",
        "        node_term = self.node_mod(x)\n",
        "#        node_term = x\n",
        "        neighbor_term = self.neighbor_mod(ax)\n",
        "        h = node_term + neighbor_term\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class GCNBinaryClassifier(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    A simple graph-convolutional network for binary classification of nodes.\n",
        "    \n",
        "    This model applies a feature extractor to each node,\n",
        "    followed by a number of graph conv layers,\n",
        "    followed by a node classifier head.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dims, gc_type=GraphConvSelfInt):\n",
        "        super(GCNBinaryClassifier, self).__init__()\n",
        "        # Feature extractor layer\n",
        "        self.feature_extractor = nn.Linear(input_dim, hidden_dims[0])\n",
        "        # Graph convolution layers\n",
        "        n_gc_layers = len(hidden_dims) - 1\n",
        "        self.gc_layers = nn.ModuleList([ gc_type(hidden_dims[i], hidden_dims[i+1]) for i in range(n_gc_layers) ])        \n",
        "        # Node classifier\n",
        "        self.classifier = nn.Linear(hidden_dims[-1], 1)\n",
        "\n",
        "        # Print NN parameters and values\n",
        "        print(\"self.feature_extractor:\")\n",
        "        for name, tensor in self.feature_extractor.named_parameters():\n",
        "          print(\"{:6s}  -  {}\".format(name, tensor.shape))\n",
        "\n",
        "    \n",
        "    def forward(self, x, a):\n",
        "        # Apply feature extraction layer\n",
        "        h = self.feature_extractor(x)\n",
        "        h = t.relu(h)\n",
        "        # Apply graph conv layers\n",
        "        for gc in self.gc_layers:\n",
        "            h = gc(h, a)\n",
        "            h = t.relu(h)\n",
        "        # Apply node classifier\n",
        "        h = self.classifier(h).squeeze(-1)\n",
        "        # Applies sigmoid activation on output to normalize to [0,1]\n",
        "        h = t.sigmoid(h)\n",
        "        return h\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "\"\"\"\n",
        "# The degree matrix is computed by summing the adjacency rows\n",
        "deg = np.diagflat(adj.sum(axis=0))\n",
        "\n",
        "# Inverse degree matrix\n",
        "invdeg = np.diagflat(1. / adj.sum(axis=0))\n",
        "\n",
        "# Normalized adjacency\n",
        "adj_norm = invdeg.dot(adj)\n",
        "In [12]:\n",
        "\n",
        "# Symmetric (+ identity) normalized adjacency\n",
        "adj_hat = np.eye(adj.shape[0]) + adj\n",
        "inv_half_deg = np.diagflat(1. / np.sqrt(adj_hat.sum(axis=0)))\n",
        "adj_hat_norm = inv_half_deg.dot(adj_hat.dot(inv_half_deg))\n",
        "\n",
        "# A*X*W\n",
        "adj.dot(X).dot(W) + b\n",
        "\n",
        "# D^-1 * A * X * W\n",
        "adj_norm.dot(X).dot(W) + b\n",
        "\n",
        "# D^(-1/2) * A * D^(-1/2) * X * W + b\n",
        "adj_hat_norm.dot(X).dot(W) + b\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VnEldJRHla"
      },
      "source": [
        "## PYTORCH module implementing the Estimator\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/estimator.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx4_Cv00RNU-"
      },
      "source": [
        "\"\"\"\n",
        "This module contains the Estimator class implementation which provides\n",
        "code for doing the training of a PyTorch model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from datetime import datetime\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import shutil \n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "def logger(s):\n",
        "    \"\"\"Simple logger function which prints date/time\"\"\"\n",
        "    print(datetime.now(), s)\n",
        "\n",
        "class Estimator():\n",
        "    \"\"\"Estimator class\"\"\"\n",
        "\n",
        "    def __init__(self, model, loss_func, opt='Adam',\n",
        "                 train_losses=None, valid_losses=None,\n",
        "                 cuda=False, l1=0.):\n",
        "\n",
        "        self.model = model\n",
        "        if cuda:\n",
        "            self.model.cuda()\n",
        "        self.loss_func = loss_func\n",
        "        if opt == 'Adam':\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        elif opt == 'SGD':\n",
        "            self.optimizer = torch.optim.SGD(self.model.parameters())\n",
        "\n",
        "        self.train_losses = train_losses if train_losses is not None else []\n",
        "        self.valid_losses = valid_losses if valid_losses is not None else []\n",
        "        self.l1 = l1\n",
        "\n",
        "        logger('Model: \\n%s' % model)\n",
        "        logger('Parameters: %i' %\n",
        "               sum(param.numel() for param in model.parameters()))\n",
        "\n",
        "    def l1_penalty(self, arr):\n",
        "        return torch.abs(arr).sum()\n",
        "        \n",
        "    def training_step(self, X , A, targets):\n",
        "        \"\"\"Applies single optimization step on batch\"\"\"\n",
        "        self.model.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "      \n",
        "        outputs = self.model(X,A)\n",
        "\n",
        "#        node_weights = [layer.weight for layer in self.model.node_network.network if hasattr(layer, 'weight')]\n",
        "#        edge_weights = [layer.weight for layer in self.model.edge_network.network if hasattr(layer, 'weight')]\n",
        "#        l1_regularization = self.l1 * sum([self.l1_penalty(arr) for arr in node_weights]) + self.l1 * sum([self.l1_penalty(arr) for arr in edge_weights])\n",
        "#        loss = self.loss_func(outputs, targets) + l1_regularization \n",
        "\n",
        "#        print(\"X=\",X.size())\n",
        "#        print(\"A=\",A.size())\n",
        "\n",
        "\n",
        "        loss = self.loss_func(outputs, targets) # no regularization for the moment \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def save_checkpoint(self, state, is_best, filename='checkpoint.pt'):\n",
        "#        directory = os.path.dirname(filename)\n",
        "        directory = data_dir\n",
        "        try:\n",
        "            os.stat(directory)\n",
        "        except:\n",
        "            os.mkdir(directory)\n",
        "        torch.save(state, filename)\n",
        "        if is_best:\n",
        "            bestfilename = directory+'/model_best.pt'\n",
        "            shutil.copyfile(filename, bestfilename)\n",
        "            \n",
        "    def load_checkpoint(self, filename='checkpoint.pt'):\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.valid_losses = checkpoint['valid_losses']\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "    \n",
        "    def fit_gen(self, train_generator, n_batches=1, n_epochs=1,\n",
        "                valid_generator=None, n_valid_batches=1, verbose=0, \n",
        "                filename='checkpoint.pt'):\n",
        "        \"\"\"Runs batch training for a number of specified epochs.\"\"\"\n",
        "        epoch_start = len(self.train_losses)\n",
        "        epoch_end = epoch_start + n_epochs\n",
        "        if len(self.valid_losses) > 0:\n",
        "            best_valid_loss = self.valid_losses[-1]\n",
        "        else:\n",
        "            best_valid_loss = 99999999\n",
        "        for i in range(epoch_start, epoch_end):\n",
        "            logger('Epoch %i' % i)\n",
        "            start_time = timer()\n",
        "            sum_loss = 0\n",
        "\n",
        "            # Train the model\n",
        "            self.model.train()\n",
        "            \n",
        "            for j in range(n_batches):\n",
        "                train_x, train_a, train_y = next(train_generator)\n",
        "                train_out = self.training_step(train_x, train_a, train_y)\n",
        "                batch_loss = train_out.cpu().data.item() \n",
        "                sum_loss += batch_loss\n",
        "                if verbose > 0:\n",
        "                    logger('  Batch %i loss %f' % (j, batch_loss))\n",
        "            end_time = timer()\n",
        "            avg_loss = sum_loss / n_batches\n",
        "            self.train_losses.append(avg_loss)\n",
        "            logger('  training loss %.3g time %gs' %\n",
        "                   (avg_loss, (end_time - start_time)))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Evaluate the model on the validation set\n",
        "                if (valid_generator is not None) and (n_valid_batches > 0):\n",
        "                    self.model.eval()\n",
        "                    valid_loss = 0\n",
        "                    for j in range(n_valid_batches):\n",
        "                        valid_x, valid_a, valid_y = next(valid_generator)\n",
        "                        valid_loss += (self.loss_func(self.model(valid_x, valid_a ), valid_y).cpu().data.item())\n",
        "                    valid_loss = valid_loss / n_valid_batches\n",
        "                    self.valid_losses.append(valid_loss)\n",
        "                    logger('  validate loss %.3g' % valid_loss)\n",
        "                \n",
        "                    #Save model checkpoint - modified\n",
        "                    logger(' save checkpoint') \n",
        "                    is_best = valid_loss < best_valid_loss\n",
        "                    best_valid_loss = min(valid_loss, best_valid_loss)\n",
        "                    self.save_checkpoint({\n",
        "                        'epoch': i + 1,\n",
        "                        'state_dict': self.model.state_dict(),\n",
        "                        'best_valid_loss': best_valid_loss,\n",
        "                        'valid_losses': self.valid_losses,\n",
        "                        'train_losses': self.train_losses,\n",
        "                        'optimizer' : self.optimizer.state_dict(),\n",
        "                    }, is_best, filename=filename)\n",
        "\n",
        "    def predict(self, generator, n_batches, concat=True):\n",
        "        with torch.no_grad():  \n",
        "            self.model.eval()\n",
        "            outputs = []\n",
        "            for j in range(n_batches):\n",
        "                test_x, test_a,  test_y = next(generator)\n",
        "                outputs.append(self.model(test_x, test_a))\n",
        "            if concat:\n",
        "                outputs = torch.cat(outputs)\n",
        "            return outputs\n",
        "\n",
        " \n",
        "\n",
        " ##############################################################################\n",
        "\n",
        "# Estimator for GCN\n",
        "'''\n",
        "def predict_prob(model, inputs):\n",
        "     return t.sigmoid(model(inputs))\n",
        "\n",
        "def training_step(model, inputs, targets, loss_func, optimizer):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    outputs = model(*inputs)\n",
        "    loss = loss_func(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def accuracy(probs, target, threshold=0.5):\n",
        "    return ((probs.data.numpy() > threshold) == (target.data.numpy() > 0.5)).mean()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxFBp9ERwT5W"
      },
      "source": [
        "## Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jU5ee5RwT5X"
      },
      "source": [
        "#def batch_generator(X, Ri, Ro, y, n_samples=1, batch_size=1, train=True):\n",
        "def batch_generator(X, A, y, n_samples=1, batch_size=1, train=True):\n",
        "    volatile = not train\n",
        "    batch_idxs = np.arange(0, n_samples, batch_size)\n",
        "    # Loop over epochs\n",
        "    while True:\n",
        "        # Loop over batches\n",
        "        for j in batch_idxs:\n",
        "# Comented lines bellow because volatile is deprecated\n",
        "#            batch_X  = np_to_torch(  X[j:j+batch_size], volatile=volatile )\n",
        "#            batch_Ri = np_to_torch( Ri[j:j+batch_size], volatile=volatile )\n",
        "#            batch_Ro = np_to_torch( Ro[j:j+batch_size], volatile=volatile )\n",
        "#            batch_y  = np_to_torch(  y[j:j+batch_size], volatile=volatile )\n",
        " \n",
        "          with torch.set_grad_enabled(train):\n",
        "            batch_X  = np_to_torch(  X[j:j+batch_size] )\n",
        "            batch_A  = np_to_torch( A[j:j+batch_size] )\n",
        "            batch_y  = np_to_torch( y[j:j+batch_size] )\n",
        "#            batch_Ri = np_to_torch( Ri[j:j+batch_size] )\n",
        "#            batch_Ro = np_to_torch( Ro[j:j+batch_size] )\n",
        "#            batch_inputs = [batch_X, batch_Ri, batch_Ro]\n",
        "            yield batch_X, batch_A , batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy95GB1YjJ6"
      },
      "source": [
        "## Network  Model and Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKxuGZBwT5Z"
      },
      "source": [
        "# Model achitecture \n",
        "n_hidden = [10,10,10,10,10] # number of features in each node of a graph convolutional hidden layer\n",
        "\n",
        "# Training config\n",
        "batch_size = 50\n",
        "n_epochs = 100\n",
        "valid_frac = 0.2\n",
        "test_frac = 0.2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8HZe-IbXJd"
      },
      "source": [
        "## Load and prepare the graphs DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040oVMpLbazl"
      },
      "source": [
        "# Load GRAPHS from inputfile\n",
        "#sparse = np.load(infile, allow_pickle=True)\n",
        "#gfile = np.load(infile)\n",
        "#garray = gfile.f.arr_0 # convert file to 2D np.ndarrays\n",
        "import glob\n",
        "#filenames =  glob.glob(data_dir+'/graphs/*.npz')\n",
        "filenames =  glob.glob('/tmp/graphs/*.npz')\n",
        "#print(\"Filenames=\",filenames)\n",
        "\n",
        "# Load the graphs into a list \n",
        "graphs = []\n",
        "for f in filenames:\n",
        "  gfile = np.load(f)\n",
        "#  graph = SparseGraph(**dict(gfile.items()))\n",
        "  graph = Graph(**dict(gfile.items()))\n",
        "  graphs.append(graph)\n",
        "\n",
        "# Get the matrix sizes in this batch\n",
        "n_graphs = len(graphs)\n",
        "n_features = graphs[0].X.shape[1] \n",
        "n_nodes    = np.array([g.X.shape[0] for g in graphs])\n",
        "n_edges    = np.array([g.y_segs.shape[0] for g in graphs])\n",
        "n_hits    = np.array([g.y_hits.shape[0] for g in graphs])\n",
        "max_nodes = n_nodes.max()\n",
        "max_edges = n_edges.max()\n",
        "\n",
        "'''\n",
        "print(\"n_graphs\",n_graphs)\n",
        "print(\"n_features\",n_features)\n",
        "print(\"n_nodes\",n_nodes)\n",
        "print(\"n_edges\",n_edges)\n",
        "print(\"n_hits\",n_hits)\n",
        "print(\"max_nodes\",max_nodes)\n",
        "print(\"max_edges\",max_edges)\n",
        "'''\n",
        "\n",
        "# Define GRAPH tensors for the full dataset\n",
        "n_samples = n_graphs\n",
        "X  = np.zeros((n_samples, max_nodes, n_features), dtype=np.float32) # node features \n",
        "Ri = np.zeros((n_samples, max_nodes, max_edges) , dtype=np.float32)  # adjacency matrix\n",
        "Ro = np.zeros((n_samples, max_nodes, max_edges) , dtype=np.float32)  #\n",
        "y_hits  = np.zeros((n_samples, max_nodes), dtype=np.float32)             # target label\n",
        "y_segs  = np.zeros((n_samples, max_edges), dtype=np.float32)             # target label\n",
        "A  = np.zeros((n_samples, max_nodes , max_nodes), dtype=np.float32 )\n",
        "\n",
        "# Loop over graphs and fill the NN input tensors ( with event # indexed  by i )\n",
        "for i, g in enumerate(graphs):\n",
        "  X[i,  :n_nodes[i]] = g.X \n",
        "  Ri[i, :n_nodes[i], :n_edges[i]] = g.Ri\n",
        "  Ro[i, :n_nodes[i], :n_edges[i]] = g.Ro\n",
        "  y_hits[ i, :n_nodes[i] ] = g.y_hits                 \n",
        "  y_segs[ i, :n_edges[i] ] = g.y_segs \n",
        "\n",
        "  # Fill the ADJACENCY matrix from edge indexes in COO representation\n",
        "  for iedge in range(max_edges):\n",
        "    ri = np.flatnonzero( Ri[i,:,iedge] )\n",
        "    ro = np.flatnonzero( Ro[i,:,iedge] )\n",
        "#    print( \"Shit ------>>>> len(ri),len(ro)= \",len(ri),len(ro) )\n",
        "#    print( \"iedge= \",iedge)\n",
        "#    print( \"ri= \",ri)\n",
        "#    print( \"ro= \",ro)\n",
        "#    print(\" \")\n",
        "    if (len(ri)==1 and len(ro)==1):  A[i,ri[0],ro[0]]=1.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coATokAiZvHF"
      },
      "source": [
        "## Partition dataset into TRAIN , TEST and VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSouxwsjwT5e"
      },
      "source": [
        "%%time\n",
        "# We round by batch_size to avoid partial batches\n",
        "n_test  = int(n_samples * test_frac)     // batch_size * batch_size\n",
        "n_valid = int(n_samples * valid_frac)    // batch_size * batch_size\n",
        "n_train = (n_samples - n_valid - n_test) // batch_size * batch_size\n",
        "n_train_batches = n_train // batch_size\n",
        "n_valid_batches = n_valid // batch_size\n",
        "n_test_batches  = n_test  // batch_size\n",
        "\n",
        "# Partition the dataset into TEST, VALIDATION and TRAIN \n",
        "#(train_X, test_X,  train_A, test_A,  train_y, test_y)  = train_test_split(X, A, y, test_size=n_test)\n",
        "#(train_X, valid_X, train_A, valid_A, train_y, valid_y) = train_test_split(X, A, y, test_size=n_valid)\n",
        "(train_X, test_X,  train_A, test_A,  train_y, test_y)  = train_test_split(X, A, y_hits, test_size=n_test)\n",
        "(train_X, valid_X, train_A, valid_A, train_y, valid_y) = train_test_split(X, A, y_hits, test_size=n_valid)\n",
        "\n",
        "# Prepare the batch_generator tortch samples\n",
        "train_batcher = batch_generator(train_X, train_A, train_y, train=True , n_samples=n_train, batch_size=batch_size)\n",
        "valid_batcher = batch_generator(valid_X, valid_A, valid_y, train=False, n_samples=n_valid, batch_size=batch_size)\n",
        "test_batcher  = batch_generator(test_X ,  test_A, test_y , train=False, n_samples=n_test , batch_size=batch_size)\n",
        "\n",
        "'''\n",
        "print('Graphs shapes:', X.shape , Ri.shape , Ro.shape , y_hits.shape)\n",
        "print('Graphs node features:', feature_names)\n",
        "print(\"n_train, n_valid, n_test  = \" , n_train, \" , \" , n_valid, \" , \" , n_test )\n",
        "print('Train shapes:', train_X.shape , train_A.shape , train_y.shape)\n",
        "print('Valid shapes:', valid_X.shape , valid_A.shape , valid_y.shape)\n",
        "print('Test shapes: ', test_X.shape  , test_A.shape  , test_y.shape)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIY5Kr2raUdE"
      },
      "source": [
        "## Construct the GNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiY4tHhQwT5k"
      },
      "source": [
        "# Construct the model\n",
        "print(\"n_features=\",n_features)\n",
        "print(\"n_hidden=\",n_hidden)\n",
        "model = GCNBinaryClassifier(n_features, n_hidden,gc_type=GraphConvSelfInt)\n",
        "loss_func = nn.BCELoss()\n",
        "estim = Estimator(model, loss_func=loss_func, cuda=cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27DD1v6dakaO"
      },
      "source": [
        "## Train the GNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGZU0dEvwT5n"
      },
      "source": [
        "estim.fit_gen(train_batcher, n_batches=n_train_batches, n_epochs=n_epochs,valid_generator=valid_batcher, n_valid_batches=n_valid_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE8UBMojwT5r"
      },
      "source": [
        "## Evaluate NN Training and Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlxS9zT7wT5t"
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score , precision_score , recall_score , precision_recall_curve , roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.legend(fontsize=10)\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "LINE_WIDTH = 2\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "# plot loss vs epoch\n",
        "ax = plt.subplot(3, 2, 1)\n",
        "ax.plot(estim.train_losses, label='training set',lw=LINE_WIDTH)\n",
        "ax.plot(estim.valid_losses, label='validation set',lw=LINE_WIDTH)\n",
        "#ax.set_ylim([0, 1])\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "'''\n",
        "# plot accuracy vs epoch\n",
        "ax = plt.subplot(3, 2, 2)\n",
        "ax.plot(estim.train_accuracy, label=='training set',lw=LINE_WIDTH)\n",
        "ax.plot(estim.valid_accuracy, label='validation set',lw=LINE_WIDTH)\n",
        "#ax.set_ylim([0, 1])\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "'''\n",
        "\n",
        "\n",
        "# Evaluate on TEST data\n",
        "test_outputs = estim.predict(test_batcher, n_test_batches, concat=False)\n",
        "test_pred = torch_to_np(estim.predict(test_batcher, n_test_batches))\n",
        "\n",
        "flat_y = test_y.flatten()\n",
        "flat_pred = test_pred.flatten()\n",
        "\n",
        "# Compute the ROC and Precisio X Recall curve\n",
        "fpr, tpr, _ = sklearn.metrics.roc_curve(flat_y, flat_pred)\n",
        "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
        "p, r, t = sklearn.metrics.precision_recall_curve(flat_y, flat_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NN discriminatnt CUT \n",
        "cut=0.5\n",
        "\n",
        "# Transform predictions into a array of entries 0,1 depending if prdiction is above cut\n",
        "y_pred = flat_y.copy()\n",
        "y_pred[y_pred >= cut]=1\n",
        "y_pred[y_pred < cut]=0\n",
        "y_true = flat_y.copy()\n",
        "w_test = np.ones(len(y_pred))\n",
        "\n",
        "print(\"y_true.shape\",y_true.shape)\n",
        "print(\"y_pred.shape\",y_pred.shape)\n",
        "print(\"w_test.shape\",w_test.shape)\n",
        "\n",
        "print(\"y_true\",y_true)\n",
        "print(\"y_pred\",y_pred)\n",
        "print(\"w_test\",w_test)\n",
        "\n",
        "accuracy  = accuracy_score(y_true, y_pred, sample_weight=w_test)\n",
        "precision = precision_score(y_true, y_pred, sample_weight=w_test)\n",
        "recall    = recall_score(y_true, y_pred, sample_weight=w_test)\n",
        "print('DNN output cut:      %.4f' % cut)\n",
        "print('Accuracy:            %.4f' % accuracy)\n",
        "print('Precision/Purity:  %.4f' % precision)\n",
        "print('Sensitivity/Recall/TPR/Signal Efficiency: %.4f' % recall)\n",
        "#print('Specificity/Selectivity/TNR/Background Efficiency: %.4f' % recall)\n",
        "\n",
        " \n",
        "\n",
        "# Plot ROC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "ax = plt.subplot(3, 2, 2)\n",
        "ax.plot(fpr, tpr, lw=LINE_WIDTH, color='cyan', label='auc = %.3f' % (roc_auc))\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', color='k', label='random chance')\n",
        "ax.set_xlim([0, 1.0])\n",
        "ax.set_ylim([0, 1.0])\n",
        "ax.set_xlabel('False Positive Rate(FPR)')\n",
        "ax.set_ylabel('True Positive Rate(TPR)')\n",
        "#ax.set_title('Receiver Operating Curve(ROC)')\n",
        "ax.legend(loc=\"lower right\")\n",
        "\n",
        "# Get model prediction for signal and background \n",
        "mask = y_true.astype(int)\n",
        "y_sig = flat_pred[mask==1]\n",
        "y_bkg = flat_pred[mask==0]\n",
        "\n",
        "ax = plt.subplot(3, 2, 3)\n",
        "X = np.linspace(0.0, 1.0, 100)\n",
        "hist_sig = ax.hist(y_sig, bins=X, label='sig',histtype='step',lw=LINE_WIDTH)\n",
        "#hist_bkg = ax.hist(Y_bkg, bins=X, label='bkg',histtype='step',lw=LINE_WIDTH)\n",
        "#ax.hist(Y_train_val, bins=X, label='bkg',histtype='step')\n",
        "ax.set_xlabel('DNN Output for SGN')\n",
        "ax.legend(prop={'size': 10})\n",
        "\n",
        "ax = plt.subplot(3, 2, 4)\n",
        "X = np.linspace(0.0, 1.0, 100)\n",
        "#hist_sig = ax.hist(Y_sig, bins=X, label='sig',histtype='step',lw=LINE_WIDTH)\n",
        "hist_bkg = ax.hist(y_bkg, bins=X, label='bkg',histtype='step',lw=LINE_WIDTH)\n",
        "#ax.hist(Y_train_val, bins=X, label='bkg',histtype='step')\n",
        "ax.set_xlabel('DNN Output for BKG')\n",
        "ax.legend(prop={'size': 10})\n",
        "\n",
        "\n",
        "\n",
        "# Plot Eff x Purity\n",
        "\n",
        "ax = plt.subplot(3, 2, 5)\n",
        "ax.plot(t, p[:-1], label='purity', lw=LINE_WIDTH)\n",
        "ax.plot(t, r[:-1], label='efficiency', lw=LINE_WIDTH)\n",
        "ax.set_xlabel('Cut on model score')\n",
        "ax.tick_params(width=2, grid_alpha=0.5)\n",
        "ax.legend()\n",
        "\n",
        "ax = plt.subplot(3, 2, 6)\n",
        "ax.plot(t,p[:-1]*r[:-1], lw=LINE_WIDTH)\n",
        "ax.set_xlabel('Cut on model score')\n",
        "ax.set_ylabel('Purity*Efficiency')\n",
        "ax.tick_params(width=2, grid_alpha=0.5)\n",
        "\n",
        "# Show plots\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvpOfvzXwT5y"
      },
      "source": [
        "## Visualize some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2Ox4tJl7wT5z"
      },
      "source": [
        "def drawGraph(X, A, y, pred): \n",
        "    # Select the i/o node features for each segment    \n",
        "    # Prepare the figure\n",
        " \n",
        "    fig, (ax0,ax1) = plt.subplots(1, 2, figsize=(12,8))\n",
        "#    cmap = plt.get_cmap('bwr_r')\n",
        "\n",
        "    # Rescale for plotting purposes\n",
        "    X=X*feature_scale\n",
        "    # HITS features \n",
        "    R=X[:,0]\n",
        "    Phi=(np.pi/180.)*X[:,1]\n",
        "    Z=X[:,2] # Don't save isMuon X[:,3] component in graph feature\n",
        "    XX=R*np.cos(Phi)\n",
        "    YY=R*np.sin(Phi)\n",
        "\n",
        "    # Transform 0 and 1 labels into color list using the dictionary \n",
        "    colordic={1:'blue' , 0:'red'}\n",
        "#    cl=[colordic.get(n) for n in X[:,3]] # use the hit target value for color\n",
        "\n",
        "    # Plot the hits\n",
        "    ax0.scatter(XX,YY, s=100., color='black' )\n",
        "    ax1.scatter(Z,R, s=100., color='black' )\n",
        "    \n",
        "    # Get the hits indices from the SGMENTS arrays\n",
        "    idx_i = find(np.rot90(Ri))[1]\n",
        "    idx_o = find(np.rot90(Ro))[1]\n",
        "  \n",
        "    # Reverse to get correct hits order as in dh_hits \n",
        "    idx_i=idx_i[::-1]\n",
        "    idx_o=idx_o[::-1]\n",
        "\n",
        "    # Get SEGMENTS begin and end hits coordinates\n",
        "    R_i=X[idx_i,0]\n",
        "    Phi_i=(np.pi/180.)*X[idx_i,1]\n",
        "    Z_i=X[idx_i,2]\n",
        "    X_i=R_i*np.cos(Phi_i)\n",
        "    Y_i=R_i*np.sin(Phi_i)\n",
        "  \n",
        "    R_o=X[idx_o,0]\n",
        "    Phi_o=(np.pi/180.)*X[idx_o,1]\n",
        "    Z_o=X[idx_o,2]\n",
        "    X_o=R_o*np.cos(Phi_o)\n",
        "    Y_o=R_o*np.sin(Phi_o)\n",
        "  \n",
        "    # Create a list of SEGMENTS colors \n",
        "#    cl=[colordic.get(n) for n in y]\n",
        "    cl=[colordic.get(np.round((n-thresh+0.5),0)) for n in pred]\n",
        "\n",
        "    # Plot segments\n",
        "\n",
        "    for j in range(len(X_i)):\n",
        "      ax0.plot([X_i[j], X_o[j]], [Y_i[j], Y_o[j]], '-', color=cl[j])\n",
        "      ax1.plot([Z_i[j], Z_o[j]], [R_i[j], R_o[j]], '-', color=cl[j])\n",
        "\n",
        "\n",
        "    # Show plots\n",
        "    ax0.set_xlabel('X')\n",
        "    ax0.set_ylabel('Y')\n",
        "    ax1.set_xlabel('Z')\n",
        "    ax1.set_ylabel('R')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Loop to draw one graph per event\n",
        "\n",
        "for i in range(n_test):\n",
        "  \n",
        "    X = test_X[i,:,:] \n",
        "    A = test_A[i,:]\n",
        "    Ro = test_Ro[i,:]\n",
        "\n",
        "    y = flat_y[i] \n",
        "    pred = flat_pred[i] \n",
        "\n",
        "    print('Y =',y)\n",
        "    print('PRED =',pred)\n",
        "    print('--------------------------------------')\n",
        "\n",
        "    print('accuracy %.3f, precision %.3f, recall %.3f' % (\n",
        "        sklearn.metrics.accuracy_score(y, pred>thresh),\n",
        "        sklearn.metrics.precision_score(y, pred>thresh),\n",
        "        sklearn.metrics.recall_score(y, pred>thresh)))\n",
        "\n",
        "    drawGraph(X, A, y , pred);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}